{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning, 2024-01\n",
    "## Tarea 4 - Algoritmos de aprendizaje por refuerzo con aproximación de funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Daniel Villar González, 201923374.  \n",
    "> Daniel Alvarez, 201911320."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Descripción de tarea**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considere el problema Cart Pole implementado en el entorno de Gymnasium descrito. El objetivo de este taller es comparar algoritmos de RL tabulares con sus contrapartes que utilizan aproximación de funciones.\n",
    "\n",
    "### ***Requerimientos***\n",
    "1. La recompensa vista en un estado terminal es cero.\n",
    "2. El siguiente estado visto por un agente en un estado terminal es igual al mismo, cumpliendo con las dinámicas del MDP.\n",
    "3. Para este caso, como existe aprendizaje, no se conocen de primera mano las probabilidades de transición, por lo que el agente debe aprender.\n",
    "4. Existen varios episodios donde se actualiza la matriz Q, con el fin de conocer los mayores valores del par estado-acción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0  Action: 1  Reward: 1.0 NextState: [ 0.01518179  0.1978054   0.0426091  -0.2937135 ]  Done: False Info: False\n",
      "Step: 1  Action: 1  Reward: 1.0 NextState: [ 0.01913789  0.3922948   0.03673483 -0.5726595 ]  Done: False Info: False\n",
      "Step: 2  Action: 0  Reward: 1.0 NextState: [ 0.02698379  0.19667754  0.02528164 -0.268634  ]  Done: False Info: False\n",
      "Step: 3  Action: 1  Reward: 1.0 NextState: [ 0.03091734  0.39142972  0.01990896 -0.5532369 ]  Done: False Info: False\n",
      "Step: 4  Action: 1  Reward: 1.0 NextState: [ 0.03874593  0.5862665   0.00884422 -0.8395814 ]  Done: False Info: False\n",
      "Step: 5  Action: 1  Reward: 1.0 NextState: [ 0.05047126  0.7812666  -0.0079474  -1.1294699 ]  Done: False Info: False\n",
      "Step: 6  Action: 0  Reward: 1.0 NextState: [ 0.0660966   0.58624965 -0.0305368  -0.8392902 ]  Done: False Info: False\n",
      "Step: 7  Action: 0  Reward: 1.0 NextState: [ 0.07782159  0.39155766 -0.0473226  -0.5563648 ]  Done: False Info: False\n",
      "Step: 8  Action: 0  Reward: 1.0 NextState: [ 0.08565275  0.19713092 -0.0584499  -0.2789588 ]  Done: False Info: False\n",
      "Step: 9  Action: 1  Reward: 1.0 NextState: [ 0.08959536  0.3930359  -0.06402908 -0.5894889 ]  Done: False Info: False\n",
      "Step: 10  Action: 0  Reward: 1.0 NextState: [ 0.09745608  0.19886616 -0.07581885 -0.31764284]  Done: False Info: False\n",
      "Step: 11  Action: 0  Reward: 1.0 NextState: [ 0.1014334   0.00490137 -0.08217171 -0.04980091]  Done: False Info: False\n",
      "Step: 12  Action: 1  Reward: 1.0 NextState: [ 0.10153143  0.20109957 -0.08316773 -0.3672365 ]  Done: False Info: False\n",
      "Step: 13  Action: 0  Reward: 1.0 NextState: [ 0.10555342  0.00725183 -0.09051246 -0.10189306]  Done: False Info: False\n",
      "Step: 14  Action: 1  Reward: 1.0 NextState: [ 0.10569846  0.20354652 -0.09255032 -0.42170414]  Done: False Info: False\n",
      "Step: 15  Action: 0  Reward: 1.0 NextState: [ 0.10976939  0.00984933 -0.1009844  -0.15957278]  Done: False Info: False\n",
      "Step: 16  Action: 1  Reward: 1.0 NextState: [ 0.10996637  0.20626116 -0.10417586 -0.48232853]  Done: False Info: False\n",
      "Step: 17  Action: 0  Reward: 1.0 NextState: [ 0.1140916   0.01275196 -0.11382243 -0.22421072]  Done: False Info: False\n",
      "Step: 18  Action: 0  Reward: 1.0 NextState: [ 0.11434664 -0.18057477 -0.11830664  0.03051133]  Done: False Info: False\n",
      "Step: 19  Action: 0  Reward: 1.0 NextState: [ 0.11073514 -0.37381893 -0.11769642  0.2836503 ]  Done: False Info: False\n",
      "Step: 20  Action: 1  Reward: 1.0 NextState: [ 0.10325877 -0.17723215 -0.11202341 -0.04371272]  Done: False Info: False\n",
      "Step: 21  Action: 0  Reward: 1.0 NextState: [ 0.09971412 -0.37058434 -0.11289766  0.2116316 ]  Done: False Info: False\n",
      "Step: 22  Action: 0  Reward: 1.0 NextState: [ 0.09230243 -0.5639263  -0.10866503  0.46667677]  Done: False Info: False\n",
      "Step: 23  Action: 0  Reward: 1.0 NextState: [ 0.08102391 -0.7573586  -0.0993315   0.7232292 ]  Done: False Info: False\n",
      "Step: 24  Action: 1  Reward: 1.0 NextState: [ 0.06587674 -0.56101334 -0.08486691  0.40100765]  Done: False Info: False\n",
      "Step: 25  Action: 0  Reward: 1.0 NextState: [ 0.05465647 -0.7548354  -0.07684676  0.66577345]  Done: False Info: False\n",
      "Step: 26  Action: 1  Reward: 1.0 NextState: [ 0.03955976 -0.55873346 -0.06353129  0.34991795]  Done: False Info: False\n",
      "Step: 27  Action: 1  Reward: 1.0 NextState: [ 0.02838509 -0.36276823 -0.05653293  0.03789746]  Done: False Info: False\n",
      "Step: 28  Action: 1  Reward: 1.0 NextState: [ 0.02112973 -0.16688307 -0.05577498 -0.27207267]  Done: False Info: False\n",
      "Step: 29  Action: 0  Reward: 1.0 NextState: [ 0.01779206 -0.36116663 -0.06121644  0.00251014]  Done: False Info: False\n",
      "Step: 30  Action: 1  Reward: 1.0 NextState: [ 0.01056873 -0.1652226  -0.06116623 -0.30884176]  Done: False Info: False\n",
      "Step: 31  Action: 1  Reward: 1.0 NextState: [ 0.00726428  0.03071512 -0.06734307 -0.62017035]  Done: False Info: False\n",
      "Step: 32  Action: 0  Reward: 1.0 NextState: [ 0.00787858 -0.16340488 -0.07974648 -0.3494343 ]  Done: False Info: False\n",
      "Step: 33  Action: 1  Reward: 1.0 NextState: [ 0.00461049  0.03275533 -0.08673516 -0.6661601 ]  Done: False Info: False\n",
      "Step: 34  Action: 1  Reward: 1.0 NextState: [ 0.00526559  0.22896972 -0.10005836 -0.9848435 ]  Done: False Info: False\n",
      "Step: 35  Action: 0  Reward: 1.0 NextState: [ 0.00984499  0.03532005 -0.11975523 -0.7251899 ]  Done: False Info: False\n",
      "Step: 36  Action: 0  Reward: 1.0 NextState: [ 0.01055139 -0.15796025 -0.13425903 -0.47246987]  Done: False Info: False\n",
      "Step: 37  Action: 1  Reward: 1.0 NextState: [ 0.00739218  0.03877704 -0.14370842 -0.8042737 ]  Done: False Info: False\n",
      "Step: 38  Action: 0  Reward: 1.0 NextState: [ 0.00816772 -0.15411308 -0.1597939  -0.5600261 ]  Done: False Info: False\n",
      "Step: 39  Action: 1  Reward: 1.0 NextState: [ 0.00508546  0.04284818 -0.17099443 -0.8984839 ]  Done: False Info: False\n",
      "Step: 40  Action: 0  Reward: 1.0 NextState: [ 0.00594243 -0.14959528 -0.1889641  -0.6640563 ]  Done: False Info: False\n",
      "Step: 41  Action: 1  Reward: 1.0 NextState: [ 0.00295052  0.04758276 -0.20224524 -1.0097839 ]  Done: False Info: False\n",
      "Step: 42  Action: 1  Reward: 1.0 NextState: [ 0.00390217  0.24474485 -0.22244091 -1.3585547 ]  Done: True Info: False\n",
      "Episode 0 terminated after 43 timesteps with total reward 43.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 1  Reward: 1.0 NextState: [ 0.01142403  0.23959965  0.03162005 -0.23722674]  Done: False Info: False\n",
      "Step: 1  Action: 1  Reward: 1.0 NextState: [ 0.01621602  0.43425593  0.02687551 -0.51977044]  Done: False Info: False\n",
      "Step: 2  Action: 1  Reward: 1.0 NextState: [ 0.02490114  0.6289894   0.0164801  -0.8038648 ]  Done: False Info: False\n",
      "Step: 3  Action: 1  Reward: 1.0 NextState: [ 3.7480928e-02  8.2388157e-01  4.0281031e-04 -1.0913184e+00]  Done: False Info: False\n",
      "Step: 4  Action: 0  Reward: 1.0 NextState: [ 0.05395856  0.6287543  -0.02142356 -0.79850906]  Done: False Info: False\n",
      "Step: 5  Action: 1  Reward: 1.0 NextState: [ 0.06653365  0.82416356 -0.03739374 -1.0978537 ]  Done: False Info: False\n",
      "Step: 6  Action: 1  Reward: 1.0 NextState: [ 0.08301692  1.0197573  -0.05935081 -1.4020305 ]  Done: False Info: False\n",
      "Step: 7  Action: 0  Reward: 1.0 NextState: [ 0.10341206  0.8254208  -0.08739142 -1.1284778 ]  Done: False Info: False\n",
      "Step: 8  Action: 0  Reward: 1.0 NextState: [ 0.11992048  0.6315454  -0.10996098 -0.864435  ]  Done: False Info: False\n",
      "Step: 9  Action: 0  Reward: 1.0 NextState: [ 0.13255139  0.43807828 -0.12724967 -0.6082504 ]  Done: False Info: False\n",
      "Step: 10  Action: 1  Reward: 1.0 NextState: [ 0.14131296  0.634728   -0.13941468 -0.9381505 ]  Done: False Info: False\n",
      "Step: 11  Action: 0  Reward: 1.0 NextState: [ 0.15400751  0.44173303 -0.1581777  -0.69232213]  Done: False Info: False\n",
      "Step: 12  Action: 1  Reward: 1.0 NextState: [ 0.16284217  0.63865465 -0.17202415 -1.0303276 ]  Done: False Info: False\n",
      "Step: 13  Action: 0  Reward: 1.0 NextState: [ 0.17561527  0.44618675 -0.1926307  -0.7962129 ]  Done: False Info: False\n",
      "Step: 14  Action: 0  Reward: 1.0 NextState: [ 0.184539    0.25415614 -0.20855495 -0.5697785 ]  Done: False Info: False\n",
      "Step: 15  Action: 0  Reward: 1.0 NextState: [ 0.18962212  0.06247487 -0.21995051 -0.3493585 ]  Done: True Info: False\n",
      "Episode 1 terminated after 16 timesteps with total reward 16.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 1  Reward: 1.0 NextState: [ 0.02729485  0.21839172 -0.01919102 -0.3051378 ]  Done: False Info: False\n",
      "Step: 1  Action: 1  Reward: 1.0 NextState: [ 0.03166269  0.41378182 -0.02529377 -0.60381085]  Done: False Info: False\n",
      "Step: 2  Action: 1  Reward: 1.0 NextState: [ 0.03993832  0.6092482  -0.03736999 -0.9043522 ]  Done: False Info: False\n",
      "Step: 3  Action: 1  Reward: 1.0 NextState: [ 0.05212329  0.8048558  -0.05545703 -1.208543  ]  Done: False Info: False\n",
      "Step: 4  Action: 0  Reward: 1.0 NextState: [ 0.06822041  0.6104923  -0.07962789 -0.9337419 ]  Done: False Info: False\n",
      "Step: 5  Action: 1  Reward: 1.0 NextState: [ 0.08043025  0.80659294 -0.09830273 -1.2503467 ]  Done: False Info: False\n",
      "Step: 6  Action: 0  Reward: 1.0 NextState: [ 0.09656211  0.6128586  -0.12330966 -0.99000263]  Done: False Info: False\n",
      "Step: 7  Action: 1  Reward: 1.0 NextState: [ 0.10881928  0.80939597 -0.14310972 -1.3187314 ]  Done: False Info: False\n",
      "Step: 8  Action: 0  Reward: 1.0 NextState: [ 0.1250072   0.61634386 -0.16948435 -1.0740443 ]  Done: False Info: False\n",
      "Step: 9  Action: 0  Reward: 1.0 NextState: [ 0.13733408  0.42381725 -0.19096524 -0.8389824 ]  Done: False Info: False\n",
      "Step: 10  Action: 0  Reward: 1.0 NextState: [ 0.14581043  0.23174389 -0.20774488 -0.61191297]  Done: False Info: False\n",
      "Step: 11  Action: 1  Reward: 1.0 NextState: [ 0.1504453   0.42907065 -0.21998315 -0.9621775 ]  Done: True Info: False\n",
      "Episode 2 terminated after 12 timesteps with total reward 12.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 0  Reward: 1.0 NextState: [-0.02321276 -0.18268204 -0.01990872  0.26024082]  Done: False Info: False\n",
      "Step: 1  Action: 0  Reward: 1.0 NextState: [-0.0268664  -0.3775142  -0.01470391  0.5465784 ]  Done: False Info: False\n",
      "Step: 2  Action: 0  Reward: 1.0 NextState: [-0.03441669 -0.5724265  -0.00377234  0.83459246]  Done: False Info: False\n",
      "Step: 3  Action: 1  Reward: 1.0 NextState: [-0.04586522 -0.37725323  0.01291951  0.5407255 ]  Done: False Info: False\n",
      "Step: 4  Action: 0  Reward: 1.0 NextState: [-0.05341028 -0.5725544   0.02373402  0.83745104]  Done: False Info: False\n",
      "Step: 5  Action: 0  Reward: 1.0 NextState: [-0.06486137 -0.7679923   0.04048304  1.1375026 ]  Done: False Info: False\n",
      "Step: 6  Action: 0  Reward: 1.0 NextState: [-0.08022122 -0.96361965  0.06323309  1.4426019 ]  Done: False Info: False\n",
      "Step: 7  Action: 0  Reward: 1.0 NextState: [-0.09949361 -1.1594605   0.09208513  1.7543542 ]  Done: False Info: False\n",
      "Step: 8  Action: 1  Reward: 1.0 NextState: [-0.12268282 -0.96549565  0.12717222  1.4916744 ]  Done: False Info: False\n",
      "Step: 9  Action: 0  Reward: 1.0 NextState: [-0.14199273 -1.1619153   0.1570057   1.8212125 ]  Done: False Info: False\n",
      "Step: 10  Action: 0  Reward: 1.0 NextState: [-0.16523103 -1.3583947   0.19342995  2.1582768 ]  Done: False Info: False\n",
      "Step: 11  Action: 1  Reward: 1.0 NextState: [-0.19239894 -1.1656257   0.2365955   1.9310304 ]  Done: True Info: False\n",
      "Episode 3 terminated after 12 timesteps with total reward 12.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 0  Reward: 1.0 NextState: [-0.00462385 -0.22589675 -0.00922934  0.29373506]  Done: False Info: False\n",
      "Step: 1  Action: 1  Reward: 1.0 NextState: [-0.00914178 -0.03064443 -0.00335464 -0.00184432]  Done: False Info: False\n",
      "Step: 2  Action: 1  Reward: 1.0 NextState: [-0.00975467  0.16452546 -0.00339153 -0.29558378]  Done: False Info: False\n",
      "Step: 3  Action: 0  Reward: 1.0 NextState: [-0.00646416 -0.03054797 -0.0093032  -0.00397242]  Done: False Info: False\n",
      "Step: 4  Action: 1  Reward: 1.0 NextState: [-0.00707512  0.16470616 -0.00938265 -0.29957604]  Done: False Info: False\n",
      "Step: 5  Action: 0  Reward: 1.0 NextState: [-0.003781   -0.0302808  -0.01537417 -0.00986693]  Done: False Info: False\n",
      "Step: 6  Action: 1  Reward: 1.0 NextState: [-0.00438661  0.16505823 -0.01557151 -0.30736068]  Done: False Info: False\n",
      "Step: 7  Action: 0  Reward: 1.0 NextState: [-0.00108545 -0.02983842 -0.02171873 -0.01962898]  Done: False Info: False\n",
      "Step: 8  Action: 1  Reward: 1.0 NextState: [-0.00168222  0.16558816 -0.0221113  -0.31908453]  Done: False Info: False\n",
      "Step: 9  Action: 0  Reward: 1.0 NextState: [ 0.00162955 -0.02921202 -0.02849299 -0.03345587]  Done: False Info: False\n",
      "Step: 10  Action: 1  Reward: 1.0 NextState: [ 0.00104531  0.1663067  -0.02916211 -0.3349907 ]  Done: False Info: False\n",
      "Step: 11  Action: 1  Reward: 1.0 NextState: [ 0.00437144  0.36183128 -0.03586193 -0.63672537]  Done: False Info: False\n",
      "Step: 12  Action: 0  Reward: 1.0 NextState: [ 0.01160807  0.16722733 -0.04859643 -0.35554823]  Done: False Info: False\n",
      "Step: 13  Action: 1  Reward: 1.0 NextState: [ 0.01495261  0.36300534 -0.0557074  -0.66315025]  Done: False Info: False\n",
      "Step: 14  Action: 0  Reward: 1.0 NextState: [ 0.02221272  0.16870084 -0.0689704  -0.38851517]  Done: False Info: False\n",
      "Step: 15  Action: 1  Reward: 1.0 NextState: [ 0.02558674  0.36473054 -0.0767407  -0.70212185]  Done: False Info: False\n",
      "Step: 16  Action: 0  Reward: 1.0 NextState: [ 0.03288135  0.17075138 -0.09078314 -0.4345491 ]  Done: False Info: False\n",
      "Step: 17  Action: 0  Reward: 1.0 NextState: [ 0.03629637 -0.02297594 -0.09947412 -0.17180835]  Done: False Info: False\n",
      "Step: 18  Action: 0  Reward: 1.0 NextState: [ 0.03583685 -0.21654385 -0.1029103   0.08791099]  Done: False Info: False\n",
      "Step: 19  Action: 1  Reward: 1.0 NextState: [ 0.03150598 -0.02010887 -0.10115207 -0.23538485]  Done: False Info: False\n",
      "Step: 20  Action: 1  Reward: 1.0 NextState: [ 0.0311038   0.17630187 -0.10585977 -0.5581831 ]  Done: False Info: False\n",
      "Step: 21  Action: 1  Reward: 1.0 NextState: [ 0.03462984  0.3727381  -0.11702343 -0.88225263]  Done: False Info: False\n",
      "Step: 22  Action: 1  Reward: 1.0 NextState: [ 0.0420846   0.56923836 -0.13466848 -1.2093135 ]  Done: False Info: False\n",
      "Step: 23  Action: 1  Reward: 1.0 NextState: [ 0.05346937  0.7658176  -0.15885475 -1.5409855 ]  Done: False Info: False\n",
      "Step: 24  Action: 1  Reward: 1.0 NextState: [ 0.06878572  0.96245307 -0.18967447 -1.8787322 ]  Done: False Info: False\n",
      "Step: 25  Action: 1  Reward: 1.0 NextState: [ 0.08803478  1.1590698  -0.22724912 -2.2237985 ]  Done: True Info: False\n",
      "Episode 4 terminated after 26 timesteps with total reward 26.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 0  Reward: 1.0 NextState: [-0.03440123 -0.21682876 -0.04091481  0.26973888]  Done: False Info: False\n",
      "Step: 1  Action: 0  Reward: 1.0 NextState: [-0.0387378  -0.4113437  -0.03552003  0.5492415 ]  Done: False Info: False\n",
      "Step: 2  Action: 1  Reward: 1.0 NextState: [-0.04696468 -0.21574125 -0.0245352   0.24558221]  Done: False Info: False\n",
      "Step: 3  Action: 0  Reward: 1.0 NextState: [-0.0512795  -0.41050434 -0.01962356  0.53042626]  Done: False Info: False\n",
      "Step: 4  Action: 1  Reward: 1.0 NextState: [-0.05948959 -0.21511193 -0.00901503  0.23162514]  Done: False Info: False\n",
      "Step: 5  Action: 1  Reward: 1.0 NextState: [-0.06379183 -0.01986232 -0.00438253 -0.06388775]  Done: False Info: False\n",
      "Step: 6  Action: 0  Reward: 1.0 NextState: [-0.06418908 -0.21492116 -0.00566028  0.22740924]  Done: False Info: False\n",
      "Step: 7  Action: 1  Reward: 1.0 NextState: [-0.0684875  -0.01971878 -0.0011121  -0.06705375]  Done: False Info: False\n",
      "Step: 8  Action: 1  Reward: 1.0 NextState: [-0.06888187  0.17541909 -0.00245317 -0.36008734]  Done: False Info: False\n",
      "Step: 9  Action: 1  Reward: 1.0 NextState: [-0.06537349  0.37057585 -0.00965492 -0.6535428 ]  Done: False Info: False\n",
      "Step: 10  Action: 1  Reward: 1.0 NextState: [-0.05796197  0.5658309  -0.02272578 -0.9492502 ]  Done: False Info: False\n",
      "Step: 11  Action: 1  Reward: 1.0 NextState: [-0.04664535  0.7612513  -0.04171078 -1.2489859 ]  Done: False Info: False\n",
      "Step: 12  Action: 0  Reward: 1.0 NextState: [-0.03142033  0.5666881  -0.0666905  -0.96965444]  Done: False Info: False\n",
      "Step: 13  Action: 0  Reward: 1.0 NextState: [-0.02008657  0.37252176 -0.08608359 -0.6986448 ]  Done: False Info: False\n",
      "Step: 14  Action: 0  Reward: 1.0 NextState: [-0.01263613  0.17869216 -0.10005648 -0.43425435]  Done: False Info: False\n",
      "Step: 15  Action: 0  Reward: 1.0 NextState: [-0.00906229 -0.01488144 -0.10874157 -0.17471373]  Done: False Info: False\n",
      "Step: 16  Action: 0  Reward: 1.0 NextState: [-0.00935992 -0.20829268 -0.11223584  0.0817825 ]  Done: False Info: False\n",
      "Step: 17  Action: 1  Reward: 1.0 NextState: [-0.01352577 -0.01175572 -0.1106002  -0.24409619]  Done: False Info: False\n",
      "Step: 18  Action: 1  Reward: 1.0 NextState: [-0.01376089  0.18475792 -0.11548212 -0.5695158 ]  Done: False Info: False\n",
      "Step: 19  Action: 0  Reward: 1.0 NextState: [-0.01006573 -0.00857119 -0.12687244 -0.31533003]  Done: False Info: False\n",
      "Step: 20  Action: 1  Reward: 1.0 NextState: [-0.01023715  0.18810815 -0.13317904 -0.6451783 ]  Done: False Info: False\n",
      "Step: 21  Action: 0  Reward: 1.0 NextState: [-0.00647499 -0.0049312  -0.14608261 -0.3972224 ]  Done: False Info: False\n",
      "Step: 22  Action: 0  Reward: 1.0 NextState: [-0.00657361 -0.1977113  -0.15402706 -0.15392792]  Done: False Info: False\n",
      "Step: 23  Action: 0  Reward: 1.0 NextState: [-0.01052784 -0.39033088 -0.15710561  0.08647575]  Done: False Info: False\n",
      "Step: 24  Action: 0  Reward: 1.0 NextState: [-0.01833446 -0.58289284 -0.15537609  0.3257621 ]  Done: False Info: False\n",
      "Step: 25  Action: 0  Reward: 1.0 NextState: [-0.02999231 -0.77550066 -0.14886086  0.5656964 ]  Done: False Info: False\n",
      "Step: 26  Action: 1  Reward: 1.0 NextState: [-0.04550232 -0.5786384  -0.13754693  0.23006512]  Done: False Info: False\n",
      "Step: 27  Action: 0  Reward: 1.0 NextState: [-0.05707509 -0.7715542  -0.13294563  0.47639441]  Done: False Info: False\n",
      "Step: 28  Action: 1  Reward: 1.0 NextState: [-0.07250617 -0.57483023 -0.12341774  0.14494145]  Done: False Info: False\n",
      "Step: 29  Action: 1  Reward: 1.0 NextState: [-0.08400278 -0.3781768  -0.12051891 -0.18398777]  Done: False Info: False\n",
      "Step: 30  Action: 1  Reward: 1.0 NextState: [-0.09156632 -0.18155506 -0.12419866 -0.51212794]  Done: False Info: False\n",
      "Step: 31  Action: 0  Reward: 1.0 NextState: [-0.09519742 -0.37472886 -0.13444121 -0.26101977]  Done: False Info: False\n",
      "Step: 32  Action: 1  Reward: 1.0 NextState: [-0.10269199 -0.17796955 -0.13966161 -0.5929023 ]  Done: False Info: False\n",
      "Step: 33  Action: 1  Reward: 1.0 NextState: [-0.10625139  0.01880265 -0.15151966 -0.92611384]  Done: False Info: False\n",
      "Step: 34  Action: 1  Reward: 1.0 NextState: [-0.10587534  0.21561012 -0.17004193 -1.2623193 ]  Done: False Info: False\n",
      "Step: 35  Action: 0  Reward: 1.0 NextState: [-0.10156313  0.02302055 -0.19528832 -1.027353  ]  Done: False Info: False\n",
      "Step: 36  Action: 0  Reward: 1.0 NextState: [-0.10110272 -0.16904198 -0.21583538 -0.80178595]  Done: True Info: False\n",
      "Episode 5 terminated after 37 timesteps with total reward 37.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 0  Reward: 1.0 NextState: [-0.0089857  -0.24065018 -0.01587347  0.2812456 ]  Done: False Info: False\n",
      "Step: 1  Action: 0  Reward: 1.0 NextState: [-0.01379871 -0.43554214 -0.01024856  0.5688801 ]  Done: False Info: False\n",
      "Step: 2  Action: 1  Reward: 1.0 NextState: [-0.02250955 -0.24027796  0.00112904  0.2729862 ]  Done: False Info: False\n",
      "Step: 3  Action: 1  Reward: 1.0 NextState: [-0.02731511 -0.04517214  0.00658877 -0.01934042]  Done: False Info: False\n",
      "Step: 4  Action: 1  Reward: 1.0 NextState: [-0.02821855  0.1498547   0.00620196 -0.30993724]  Done: False Info: False\n",
      "Step: 5  Action: 1  Reward: 1.0 NextState: [-2.5221458e-02  3.4488773e-01  3.2127084e-06 -6.0065782e-01]  Done: False Info: False\n",
      "Step: 6  Action: 0  Reward: 1.0 NextState: [-0.0183237   0.14976574 -0.01200994 -0.3079739 ]  Done: False Info: False\n",
      "Step: 7  Action: 1  Reward: 1.0 NextState: [-0.01532839  0.34505674 -0.01816942 -0.6044201 ]  Done: False Info: False\n",
      "Step: 8  Action: 1  Reward: 1.0 NextState: [-0.00842725  0.54042804 -0.03025782 -0.90277016]  Done: False Info: False\n",
      "Step: 9  Action: 0  Reward: 1.0 NextState: [ 0.00238131  0.34572873 -0.04831323 -0.61974937]  Done: False Info: False\n",
      "Step: 10  Action: 0  Reward: 1.0 NextState: [ 0.00929588  0.15131366 -0.06070821 -0.34266558]  Done: False Info: False\n",
      "Step: 11  Action: 1  Reward: 1.0 NextState: [ 0.01232216  0.3472444  -0.06756152 -0.6538575 ]  Done: False Info: False\n",
      "Step: 12  Action: 1  Reward: 1.0 NextState: [ 0.01926704  0.5432388  -0.08063868 -0.9670264 ]  Done: False Info: False\n",
      "Step: 13  Action: 1  Reward: 1.0 NextState: [ 0.03013182  0.73934567 -0.09997921 -1.2839129 ]  Done: False Info: False\n",
      "Step: 14  Action: 0  Reward: 1.0 NextState: [ 0.04491873  0.5456287  -0.12565747 -1.0241334 ]  Done: False Info: False\n",
      "Step: 15  Action: 0  Reward: 1.0 NextState: [ 0.05583131  0.3523838  -0.14614013 -0.77339774]  Done: False Info: False\n",
      "Step: 16  Action: 0  Reward: 1.0 NextState: [ 0.06287898  0.15954223 -0.16160809 -0.5300312 ]  Done: False Info: False\n",
      "Step: 17  Action: 0  Reward: 1.0 NextState: [ 0.06606983 -0.03298154 -0.17220871 -0.29231468]  Done: False Info: False\n",
      "Step: 18  Action: 0  Reward: 1.0 NextState: [ 0.0654102  -0.22528341 -0.178055   -0.05850796]  Done: False Info: False\n",
      "Step: 19  Action: 0  Reward: 1.0 NextState: [ 0.06090453 -0.41746488 -0.17922516  0.17313468]  Done: False Info: False\n",
      "Step: 20  Action: 0  Reward: 1.0 NextState: [ 0.05255523 -0.6096296  -0.17576247  0.404354  ]  Done: False Info: False\n",
      "Step: 21  Action: 0  Reward: 1.0 NextState: [ 0.04036264 -0.8018802  -0.16767539  0.6368786 ]  Done: False Info: False\n",
      "Step: 22  Action: 0  Reward: 1.0 NextState: [ 0.02432504 -0.9943161  -0.15493782  0.8724183 ]  Done: False Info: False\n",
      "Step: 23  Action: 1  Reward: 1.0 NextState: [ 0.00443871 -0.79746515 -0.13748945  0.5353092 ]  Done: False Info: False\n",
      "Step: 24  Action: 0  Reward: 1.0 NextState: [-0.01151059 -0.9904131  -0.12678327  0.7817054 ]  Done: False Info: False\n",
      "Step: 25  Action: 0  Reward: 1.0 NextState: [-0.03131885 -1.1835856  -0.11114916  1.031964  ]  Done: False Info: False\n",
      "Step: 26  Action: 0  Reward: 1.0 NextState: [-0.05499056 -1.3770677  -0.09050988  1.2877855 ]  Done: False Info: False\n",
      "Step: 27  Action: 0  Reward: 1.0 NextState: [-0.08253192 -1.570929   -0.06475417  1.5508137 ]  Done: False Info: False\n",
      "Step: 28  Action: 1  Reward: 1.0 NextState: [-0.1139505  -1.3750929  -0.03373789  1.2386507 ]  Done: False Info: False\n",
      "Step: 29  Action: 0  Reward: 1.0 NextState: [-0.14145236 -1.5697656  -0.00896488  1.5205766 ]  Done: False Info: False\n",
      "Step: 30  Action: 1  Reward: 1.0 NextState: [-0.17284767 -1.3745365   0.02144665  1.225109  ]  Done: False Info: False\n",
      "Step: 31  Action: 0  Reward: 1.0 NextState: [-0.2003384  -1.5699279   0.04594883  1.5244336 ]  Done: False Info: False\n",
      "Step: 32  Action: 0  Reward: 1.0 NextState: [-0.23173696 -1.7655736   0.0764375   1.8310965 ]  Done: False Info: False\n",
      "Step: 33  Action: 1  Reward: 1.0 NextState: [-0.26704842 -1.5713767   0.11305943  1.5631025 ]  Done: False Info: False\n",
      "Step: 34  Action: 1  Reward: 1.0 NextState: [-0.29847595 -1.3777738   0.14432149  1.3077208 ]  Done: False Info: False\n",
      "Step: 35  Action: 0  Reward: 1.0 NextState: [-0.32603145 -1.5743994   0.1704759   1.6418763 ]  Done: False Info: False\n",
      "Step: 36  Action: 1  Reward: 1.0 NextState: [-0.35751942 -1.381635    0.20331343  1.4067986 ]  Done: False Info: False\n",
      "Step: 37  Action: 1  Reward: 1.0 NextState: [-0.38515213 -1.1895323   0.2314494   1.183943  ]  Done: True Info: False\n",
      "Episode 6 terminated after 38 timesteps with total reward 38.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 1  Reward: 1.0 NextState: [ 0.02203693  0.15321667 -0.01872988 -0.31955114]  Done: False Info: False\n",
      "Step: 1  Action: 1  Reward: 1.0 NextState: [ 0.02510126  0.3486003  -0.02512091 -0.61808145]  Done: False Info: False\n",
      "Step: 2  Action: 0  Reward: 1.0 NextState: [ 0.03207327  0.1538381  -0.03748253 -0.3334151 ]  Done: False Info: False\n",
      "Step: 3  Action: 1  Reward: 1.0 NextState: [ 0.03515003  0.34947294 -0.04415084 -0.6376785 ]  Done: False Info: False\n",
      "Step: 4  Action: 0  Reward: 1.0 NextState: [ 0.04213949  0.15499356 -0.05690441 -0.35921985]  Done: False Info: False\n",
      "Step: 5  Action: 1  Reward: 1.0 NextState: [ 0.04523936  0.35087633 -0.06408881 -0.6692893 ]  Done: False Info: False\n",
      "Step: 6  Action: 1  Reward: 1.0 NextState: [ 0.05225689  0.5468281  -0.07747459 -0.9814427 ]  Done: False Info: False\n",
      "Step: 7  Action: 1  Reward: 1.0 NextState: [ 0.06319345  0.74289805 -0.09710345 -1.2974201 ]  Done: False Info: False\n",
      "Step: 8  Action: 1  Reward: 1.0 NextState: [ 0.07805141  0.9391097  -0.12305184 -1.6188548 ]  Done: False Info: False\n",
      "Step: 9  Action: 1  Reward: 1.0 NextState: [ 0.0968336   1.1354483  -0.15542895 -1.9472219 ]  Done: False Info: False\n",
      "Step: 10  Action: 1  Reward: 1.0 NextState: [ 0.11954257  1.3318466  -0.19437338 -2.2837803 ]  Done: False Info: False\n",
      "Step: 11  Action: 1  Reward: 1.0 NextState: [ 0.1461795   1.5281677  -0.24004899 -2.6295033 ]  Done: True Info: False\n",
      "Episode 7 terminated after 12 timesteps with total reward 12.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 0  Reward: 1.0 NextState: [-0.0089131  -0.1502052   0.01595551  0.33833295]  Done: False Info: False\n",
      "Step: 1  Action: 0  Reward: 1.0 NextState: [-0.01191721 -0.3455505   0.02272217  0.6360044 ]  Done: False Info: False\n",
      "Step: 2  Action: 1  Reward: 1.0 NextState: [-0.01882822 -0.15075271  0.03544226  0.35056284]  Done: False Info: False\n",
      "Step: 3  Action: 1  Reward: 1.0 NextState: [-0.02184327  0.04384774  0.04245352  0.06926331]  Done: False Info: False\n",
      "Step: 4  Action: 1  Reward: 1.0 NextState: [-0.02096632  0.23833615  0.04383878 -0.20972885]  Done: False Info: False\n",
      "Step: 5  Action: 1  Reward: 1.0 NextState: [-0.01619959  0.43280473  0.0396442  -0.488267  ]  Done: False Info: False\n",
      "Step: 6  Action: 1  Reward: 1.0 NextState: [-0.0075435   0.62734556  0.02987886 -0.76819664]  Done: False Info: False\n",
      "Step: 7  Action: 0  Reward: 1.0 NextState: [ 0.00500341  0.43182534  0.01451493 -0.46626413]  Done: False Info: False\n",
      "Step: 8  Action: 1  Reward: 1.0 NextState: [ 0.01363992  0.62673926  0.00518965 -0.75433695]  Done: False Info: False\n",
      "Step: 9  Action: 0  Reward: 1.0 NextState: [ 0.02617471  0.43154612 -0.00989709 -0.46002546]  Done: False Info: False\n",
      "Step: 10  Action: 0  Reward: 1.0 NextState: [ 0.03480563  0.23656546 -0.0190976  -0.17047848]  Done: False Info: False\n",
      "Step: 11  Action: 1  Reward: 1.0 NextState: [ 0.03953694  0.4319555  -0.02250717 -0.4691244 ]  Done: False Info: False\n",
      "Step: 12  Action: 1  Reward: 1.0 NextState: [ 0.04817605  0.627388   -0.03188966 -0.7688155 ]  Done: False Info: False\n",
      "Step: 13  Action: 0  Reward: 1.0 NextState: [ 0.06072381  0.4327192  -0.04726597 -0.48633474]  Done: False Info: False\n",
      "Step: 14  Action: 1  Reward: 1.0 NextState: [ 0.06937819  0.6284751  -0.05699266 -0.7935317 ]  Done: False Info: False\n",
      "Step: 15  Action: 1  Reward: 1.0 NextState: [ 0.08194769  0.82433116 -0.0728633  -1.1035856 ]  Done: False Info: False\n",
      "Step: 16  Action: 1  Reward: 1.0 NextState: [ 0.09843431  1.0203319  -0.09493501 -1.4182094 ]  Done: False Info: False\n",
      "Step: 17  Action: 1  Reward: 1.0 NextState: [ 0.11884096  1.2164922  -0.1232992  -1.7389939 ]  Done: False Info: False\n",
      "Step: 18  Action: 0  Reward: 1.0 NextState: [ 0.1431708   1.022972   -0.15807907 -1.4870756 ]  Done: False Info: False\n",
      "Step: 19  Action: 0  Reward: 1.0 NextState: [ 0.16363025  0.8300897  -0.18782058 -1.2476414 ]  Done: False Info: False\n",
      "Step: 20  Action: 1  Reward: 1.0 NextState: [ 0.18023203  1.0270563  -0.21277341 -1.5927906 ]  Done: True Info: False\n",
      "Episode 8 terminated after 21 timesteps with total reward 21.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 0  Reward: 1.0 NextState: [-0.02266727 -0.17260368 -0.00647382  0.2730723 ]  Done: False Info: False\n",
      "Step: 1  Action: 0  Reward: 1.0 NextState: [-0.02611934 -0.36763266 -0.00101237  0.56370634]  Done: False Info: False\n",
      "Step: 2  Action: 0  Reward: 1.0 NextState: [-0.03347199 -0.5627404   0.01026176  0.85607016]  Done: False Info: False\n",
      "Step: 3  Action: 0  Reward: 1.0 NextState: [-0.0447268  -0.7580007   0.02738316  1.151962  ]  Done: False Info: False\n",
      "Step: 4  Action: 1  Reward: 1.0 NextState: [-0.05988682 -0.5632465   0.0504224   0.8679899 ]  Done: False Info: False\n",
      "Step: 5  Action: 0  Reward: 1.0 NextState: [-0.07115175 -0.7590169   0.0677822   1.1760902 ]  Done: False Info: False\n",
      "Step: 6  Action: 0  Reward: 1.0 NextState: [-0.08633208 -0.9549508   0.091304    1.489229  ]  Done: False Info: False\n",
      "Step: 7  Action: 0  Reward: 1.0 NextState: [-0.10543109 -1.1510583   0.12108858  1.808971  ]  Done: False Info: False\n",
      "Step: 8  Action: 1  Reward: 1.0 NextState: [-0.12845227 -0.9574771   0.157268    1.5562384 ]  Done: False Info: False\n",
      "Step: 9  Action: 1  Reward: 1.0 NextState: [-0.14760181 -0.7645498   0.18839277  1.3164654 ]  Done: False Info: False\n",
      "Step: 10  Action: 1  Reward: 1.0 NextState: [-0.1628928  -0.5722431   0.21472208  1.0881696 ]  Done: True Info: False\n",
      "Episode 9 terminated after 11 timesteps with total reward 11.0\n",
      "El episodio no se completó exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def Random_games():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    # Each of this episode is its own game.\n",
    "    for episode in range(10):\n",
    "        env.reset()\n",
    "        total_reward = 0\n",
    "        # this is each frame, up to 500...but we wont make it that far with random.\n",
    "        for t in range(500):\n",
    "            # This will display the environment\n",
    "            # Only display if you really want to see it.\n",
    "            # Takes much longer to display it.\n",
    "            env.render()\n",
    "            \n",
    "            # This will just create a sample action in any environment.\n",
    "            # In this environment, the action can be 0 or 1, which is left or right\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # this executes the environment with an action, \n",
    "            # and returns the observation of the environment, \n",
    "            # the reward, if the env is over, and other info.\n",
    "            #print(env.step(action))\n",
    "            next_state, reward, done, info, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            # lets print everything in one line:\n",
    "             # Print information\n",
    "            print(\"Step:\", t, \" Action:\", action, \" Reward:\", reward, \"NextState:\", next_state, \" Done:\", done, \"Info:\", info)\n",
    "            \n",
    "            if done:\n",
    "                print(\"Episode\", episode, \"terminated after\", t+1, \"timesteps with total reward\", total_reward)\n",
    "                if total_reward>=500:\n",
    "                    print(\"El episodio se ha completado de manera exitosa.\")\n",
    "                else:\n",
    "                    print(\"El episodio no se completó exitosamente.\")\n",
    "                break  # Exit the loop if the episode is done\n",
    "                \n",
    "Random_games()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'dict' and 'dict'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m sarsa_agent \u001b[38;5;241m=\u001b[39m SarsaAgent(num_states, num_actions)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Entrenar el agente SARSA\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m \u001b[43mtrain_sarsa\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msarsa_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Obtener la política del agente\u001b[39;00m\n\u001b[0;32m     68\u001b[0m policy \u001b[38;5;241m=\u001b[39m get_policy(sarsa_agent\u001b[38;5;241m.\u001b[39mq_table)\n",
      "Cell \u001b[1;32mIn[13], line 40\u001b[0m, in \u001b[0;36mtrain_sarsa\u001b[1;34m(env, agent, num_episodes)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m     39\u001b[0m     state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 40\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mdiscretize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[0;32m     42\u001b[0m     terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m, in \u001b[0;36mdiscretize_state\u001b[1;34m(state, num_states)\u001b[0m\n\u001b[0;32m     28\u001b[0m num_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(num_states \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m)))  \u001b[38;5;66;03m# Calcular el número de bins en base a la raíz cuarta del número de estados\u001b[39;00m\n\u001b[0;32m     29\u001b[0m bins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4.8\u001b[39m, \u001b[38;5;241m4.8\u001b[39m, num_bins),          \u001b[38;5;66;03m# Car position\u001b[39;00m\n\u001b[0;32m     30\u001b[0m                  np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, num_bins),              \u001b[38;5;66;03m# Car velocity\u001b[39;00m\n\u001b[0;32m     31\u001b[0m                  np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.418\u001b[39m, \u001b[38;5;241m0.418\u001b[39m, num_bins),     \u001b[38;5;66;03m# Pole angle\u001b[39;00m\n\u001b[0;32m     32\u001b[0m                  np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, num_bins)])            \u001b[38;5;66;03m# Pole velocity\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m discretized_state \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigitize\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(discretized_state)\n",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     28\u001b[0m num_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(num_states \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m)))  \u001b[38;5;66;03m# Calcular el número de bins en base a la raíz cuarta del número de estados\u001b[39;00m\n\u001b[0;32m     29\u001b[0m bins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4.8\u001b[39m, \u001b[38;5;241m4.8\u001b[39m, num_bins),          \u001b[38;5;66;03m# Car position\u001b[39;00m\n\u001b[0;32m     30\u001b[0m                  np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, num_bins),              \u001b[38;5;66;03m# Car velocity\u001b[39;00m\n\u001b[0;32m     31\u001b[0m                  np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.418\u001b[39m, \u001b[38;5;241m0.418\u001b[39m, num_bins),     \u001b[38;5;66;03m# Pole angle\u001b[39;00m\n\u001b[0;32m     32\u001b[0m                  np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, num_bins)])            \u001b[38;5;66;03m# Pole velocity\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m discretized_state \u001b[38;5;241m=\u001b[39m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigitize\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(state)]\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(discretized_state)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdigitize\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:5614\u001b[0m, in \u001b[0;36mdigitize\u001b[1;34m(x, bins, right)\u001b[0m\n\u001b[0;32m   5612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bins) \u001b[38;5;241m-\u001b[39m _nx\u001b[38;5;241m.\u001b[39msearchsorted(bins[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], x, side\u001b[38;5;241m=\u001b[39mside)\n\u001b[0;32m   5613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 5614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1413\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(a, v, side, sorter)\u001b[0m\n\u001b[0;32m   1345\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[0;32m   1349\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1411\u001b[0m \n\u001b[0;32m   1412\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearchsorted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:66\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class SarsaAgent:\n",
    "    def __init__(self, num_states, num_actions, discount_factor=0.99, alpha=0.1, epsilon=0.1):\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "        self.discount_factor = discount_factor\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_init = epsilon\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(len(self.q_table[state]))\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state, next_action):\n",
    "        predict = self.q_table[state, action]\n",
    "        target = reward + self.discount_factor * self.q_table[next_state, next_action]\n",
    "        self.q_table[state, action] += self.alpha * (target - predict)\n",
    "\n",
    "    def reset_epsilon(self):\n",
    "        self.epsilon = self.epsilon_init\n",
    "\n",
    "# Función para discretizar el espacio de estados\n",
    "def discretize_state(state, num_states):\n",
    "    num_bins = int(round(num_states ** (1/4)))  # Calcular el número de bins en base a la raíz cuarta del número de estados\n",
    "    bins = np.array([np.linspace(-4.8, 4.8, num_bins),          # Car position\n",
    "                     np.linspace(-5, 5, num_bins),              # Car velocity\n",
    "                     np.linspace(-0.418, 0.418, num_bins),     # Pole angle\n",
    "                     np.linspace(-5, 5, num_bins)])            # Pole velocity\n",
    "    discretized_state = [np.digitize(s, bins[i]) - 1 for i, s in enumerate(state)]\n",
    "    return tuple(discretized_state)\n",
    "\n",
    "# Función para entrenar el agente SARSA\n",
    "def train_sarsa(env, agent, num_episodes):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = discretize_state(state, num_states)\n",
    "        action = agent.choose_action(state)\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            next_state = discretize_state(next_state, num_states)\n",
    "            next_action = agent.choose_action(next_state)\n",
    "            agent.update_q_table(state, action, reward, next_state, next_action)\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        agent.reset_epsilon()\n",
    "\n",
    "# Función para obtener la política del agente\n",
    "def get_policy(q_table):\n",
    "    return np.argmax(q_table, axis=1)\n",
    "\n",
    "# Crear el entorno CartPole-v1\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Definir el agente SARSA\n",
    "num_states = 300\n",
    "num_actions = env.action_space.n\n",
    "sarsa_agent = SarsaAgent(num_states, num_actions)\n",
    "\n",
    "# Entrenar el agente SARSA\n",
    "train_sarsa(env, sarsa_agent, num_episodes=10000)\n",
    "\n",
    "# Obtener la política del agente\n",
    "policy = get_policy(sarsa_agent.q_table)\n",
    "print(\"Política aprendida:\", policy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
