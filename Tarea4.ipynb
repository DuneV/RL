{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning, 2024-01\n",
    "## Tarea 4 - Algoritmos de aprendizaje por refuerzo con aproximación de funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Daniel Villar González, 201923374.  \n",
    "> Daniel Alvarez, 201911320."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Descripción de tarea**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considere el problema Cart Pole implementado en el entorno de Gymnasium descrito. El objetivo de este taller es comparar algoritmos de RL tabulares con sus contrapartes que utilizan aproximación de funciones.\n",
    "\n",
    "### ***Requerimientos***\n",
    "1. La recompensa vista en un estado terminal es cero.\n",
    "2. El siguiente estado visto por un agente en un estado terminal es igual al mismo, cumpliendo con las dinámicas del MDP.\n",
    "3. Para este caso, como existe aprendizaje, no se conocen de primera mano las probabilidades de transición, por lo que el agente debe aprender.\n",
    "4. Existen varios episodios donde se actualiza la matriz Q, con el fin de conocer los mayores valores del par estado-acción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0  Action: 0  Reward: 1.0 NextState: [ 0.01864006 -0.22263311  0.02551355  0.31297848]  Done: False Info: False\n",
      "Step: 1  Action: 0  Reward: 1.0 NextState: [ 0.0141874  -0.41810906  0.03177312  0.61359715]  Done: False Info: False\n",
      "Step: 2  Action: 0  Reward: 1.0 NextState: [ 0.00582522 -0.6136603   0.04404506  0.91611564]  Done: False Info: False\n",
      "Step: 3  Action: 0  Reward: 1.0 NextState: [-0.00644799 -0.80934924  0.06236737  1.2223095 ]  Done: False Info: False\n",
      "Step: 4  Action: 1  Reward: 1.0 NextState: [-0.02263497 -0.6150838   0.08681356  0.949802  ]  Done: False Info: False\n",
      "Step: 5  Action: 1  Reward: 1.0 NextState: [-0.03493665 -0.42123094  0.1058096   0.6856089 ]  Done: False Info: False\n",
      "Step: 6  Action: 0  Reward: 1.0 NextState: [-0.04336127 -0.61765033  0.11952178  1.0096402 ]  Done: False Info: False\n",
      "Step: 7  Action: 1  Reward: 1.0 NextState: [-0.05571428 -0.42430878  0.13971458  0.75675267]  Done: False Info: False\n",
      "Step: 8  Action: 1  Reward: 1.0 NextState: [-0.06420045 -0.23136055  0.15484963  0.51109314]  Done: False Info: False\n",
      "Step: 9  Action: 1  Reward: 1.0 NextState: [-0.06882766 -0.03872003  0.1650715   0.27093393]  Done: False Info: False\n",
      "Step: 10  Action: 1  Reward: 1.0 NextState: [-0.06960206  0.1537089   0.17049018  0.0345251 ]  Done: False Info: False\n",
      "Step: 11  Action: 1  Reward: 1.0 NextState: [-0.06652788  0.346028    0.17118068 -0.19988945]  Done: False Info: False\n",
      "Step: 12  Action: 0  Reward: 1.0 NextState: [-0.05960732  0.14892387  0.1671829   0.1415272 ]  Done: False Info: False\n",
      "Step: 13  Action: 0  Reward: 1.0 NextState: [-0.05662885 -0.04814874  0.17001344  0.4819377 ]  Done: False Info: False\n",
      "Step: 14  Action: 1  Reward: 1.0 NextState: [-0.05759182  0.14421707  0.17965218  0.24729264]  Done: False Info: False\n",
      "Step: 15  Action: 1  Reward: 1.0 NextState: [-0.05470748  0.33637935  0.18459804  0.01622234]  Done: False Info: False\n",
      "Step: 16  Action: 0  Reward: 1.0 NextState: [-0.04797989  0.13915597  0.18492249  0.36099532]  Done: False Info: False\n",
      "Step: 17  Action: 1  Reward: 1.0 NextState: [-0.04519677  0.33123413  0.1921424   0.13184822]  Done: False Info: False\n",
      "Step: 18  Action: 1  Reward: 1.0 NextState: [-0.03857209  0.5231589   0.19477937 -0.09459818]  Done: False Info: False\n",
      "Step: 19  Action: 0  Reward: 1.0 NextState: [-0.02810891  0.32585618  0.1928874   0.25266328]  Done: False Info: False\n",
      "Step: 20  Action: 1  Reward: 1.0 NextState: [-0.02159179  0.5177759   0.19794066  0.02648043]  Done: False Info: False\n",
      "Step: 21  Action: 1  Reward: 1.0 NextState: [-0.01123627  0.70959     0.19847026 -0.19780734]  Done: False Info: False\n",
      "Step: 22  Action: 1  Reward: 1.0 NextState: [ 0.00295553  0.90140164  0.19451413 -0.4219088 ]  Done: False Info: False\n",
      "Step: 23  Action: 0  Reward: 1.0 NextState: [ 0.02098356  0.7041328   0.18607594 -0.0747585 ]  Done: False Info: False\n",
      "Step: 24  Action: 0  Reward: 1.0 NextState: [0.03506622 0.50689834 0.18458077 0.27037725]  Done: False Info: False\n",
      "Step: 25  Action: 1  Reward: 1.0 NextState: [0.04520418 0.6989725  0.18998833 0.04111917]  Done: False Info: False\n",
      "Step: 26  Action: 0  Reward: 1.0 NextState: [0.05918363 0.50170606 0.19081071 0.38721576]  Done: False Info: False\n",
      "Step: 27  Action: 0  Reward: 1.0 NextState: [0.06921776 0.3044604  0.19855502 0.733473  ]  Done: False Info: False\n",
      "Step: 28  Action: 1  Reward: 1.0 NextState: [0.07530697 0.49636623 0.21322449 0.5092623 ]  Done: True Info: False\n",
      "Episode 0 terminated after 29 timesteps with total reward 29.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 1  Reward: 1.0 NextState: [ 0.03106114  0.22927944  0.02809012 -0.24542214]  Done: False Info: False\n",
      "Step: 1  Action: 0  Reward: 1.0 NextState: [0.03564673 0.03376779 0.02318168 0.05598706]  Done: False Info: False\n",
      "Step: 2  Action: 0  Reward: 1.0 NextState: [ 0.03632208 -0.16167875  0.02430142  0.3558929 ]  Done: False Info: False\n",
      "Step: 3  Action: 1  Reward: 1.0 NextState: [0.03308851 0.03308941 0.03141928 0.07097084]  Done: False Info: False\n",
      "Step: 4  Action: 0  Reward: 1.0 NextState: [ 0.0337503  -0.16246857  0.03283869  0.37339878]  Done: False Info: False\n",
      "Step: 5  Action: 0  Reward: 1.0 NextState: [ 0.03050092 -0.35804123  0.04030667  0.6762525 ]  Done: False Info: False\n",
      "Step: 6  Action: 0  Reward: 1.0 NextState: [ 0.0233401  -0.5536994   0.05383172  0.9813483 ]  Done: False Info: False\n",
      "Step: 7  Action: 1  Reward: 1.0 NextState: [ 0.01226611 -0.3593386   0.07345869  0.7060483 ]  Done: False Info: False\n",
      "Step: 8  Action: 1  Reward: 1.0 NextState: [ 0.00507934 -0.16530715  0.08757965  0.43736348]  Done: False Info: False\n",
      "Step: 9  Action: 1  Reward: 1.0 NextState: [0.0017732  0.02847304 0.09632692 0.17352274]  Done: False Info: False\n",
      "Step: 10  Action: 0  Reward: 1.0 NextState: [ 0.00234266 -0.16788612  0.09979738  0.49497238]  Done: False Info: False\n",
      "Step: 11  Action: 0  Reward: 1.0 NextState: [-0.00101506 -0.3642634   0.10969683  0.8173644 ]  Done: False Info: False\n",
      "Step: 12  Action: 1  Reward: 1.0 NextState: [-0.00830033 -0.17080036  0.12604411  0.5611003 ]  Done: False Info: False\n",
      "Step: 13  Action: 1  Reward: 1.0 NextState: [-0.01171634  0.02234829  0.13726611  0.31063467]  Done: False Info: False\n",
      "Step: 14  Action: 1  Reward: 1.0 NextState: [-0.01126937  0.21527484  0.14347881  0.06419652]  Done: False Info: False\n",
      "Step: 15  Action: 1  Reward: 1.0 NextState: [-0.00696388  0.40807936  0.14476274 -0.18000035]  Done: False Info: False\n",
      "Step: 16  Action: 0  Reward: 1.0 NextState: [0.00119771 0.21121451 0.14116274 0.15461992]  Done: False Info: False\n",
      "Step: 17  Action: 0  Reward: 1.0 NextState: [0.005422   0.01438325 0.14425513 0.48829418]  Done: False Info: False\n",
      "Step: 18  Action: 0  Reward: 1.0 NextState: [ 0.00570967 -0.18244788  0.15402101  0.8227383 ]  Done: False Info: False\n",
      "Step: 19  Action: 1  Reward: 1.0 NextState: [0.00206071 0.01026938 0.17047578 0.5821878 ]  Done: False Info: False\n",
      "Step: 20  Action: 0  Reward: 1.0 NextState: [ 0.0022661  -0.18677928  0.18211953  0.9233536 ]  Done: False Info: False\n",
      "Step: 21  Action: 0  Reward: 1.0 NextState: [-0.00146949 -0.3838321   0.20058662  1.2672923 ]  Done: False Info: False\n",
      "Step: 22  Action: 1  Reward: 1.0 NextState: [-0.00914613 -0.19175605  0.22593245  1.0435327 ]  Done: True Info: False\n",
      "Episode 1 terminated after 23 timesteps with total reward 23.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 0  Reward: 1.0 NextState: [ 0.04345683 -0.20944895  0.04141759  0.26721725]  Done: False Info: False\n",
      "Step: 1  Action: 0  Reward: 1.0 NextState: [ 0.03926785 -0.4051368   0.04676194  0.5726706 ]  Done: False Info: False\n",
      "Step: 2  Action: 0  Reward: 1.0 NextState: [ 0.03116511 -0.6008821   0.05821535  0.8797106 ]  Done: False Info: False\n",
      "Step: 3  Action: 1  Reward: 1.0 NextState: [ 0.01914747 -0.40659738  0.07580956  0.6058828 ]  Done: False Info: False\n",
      "Step: 4  Action: 0  Reward: 1.0 NextState: [ 0.01101552 -0.60269296  0.08792722  0.92144805]  Done: False Info: False\n",
      "Step: 5  Action: 1  Reward: 1.0 NextState: [-0.00103834 -0.40886217  0.10635618  0.6576423 ]  Done: False Info: False\n",
      "Step: 6  Action: 0  Reward: 1.0 NextState: [-0.00921558 -0.6052912   0.11950903  0.9818308 ]  Done: False Info: False\n",
      "Step: 7  Action: 0  Reward: 1.0 NextState: [-0.02132141 -0.8017943   0.13914564  1.3095351 ]  Done: False Info: False\n",
      "Step: 8  Action: 1  Reward: 1.0 NextState: [-0.03735729 -0.608682    0.16533634  1.0634433 ]  Done: False Info: False\n",
      "Step: 9  Action: 1  Reward: 1.0 NextState: [-0.04953093 -0.41608837  0.18660522  0.82688016]  Done: False Info: False\n",
      "Step: 10  Action: 1  Reward: 1.0 NextState: [-0.0578527  -0.22394109  0.20314282  0.59820694]  Done: False Info: False\n",
      "Step: 11  Action: 0  Reward: 1.0 NextState: [-0.06233152 -0.42123887  0.21510695  0.9473822 ]  Done: True Info: False\n",
      "Episode 2 terminated after 12 timesteps with total reward 12.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 1  Reward: 1.0 NextState: [ 0.00905124  0.16820423  0.01540336 -0.3153977 ]  Done: False Info: False\n",
      "Step: 1  Action: 1  Reward: 1.0 NextState: [ 0.01241533  0.36310342  0.0090954  -0.6031834 ]  Done: False Info: False\n",
      "Step: 2  Action: 0  Reward: 1.0 NextState: [ 0.0196774   0.16785544 -0.00296827 -0.30764955]  Done: False Info: False\n",
      "Step: 3  Action: 0  Reward: 1.0 NextState: [ 0.02303451 -0.02722409 -0.00912126 -0.01590421]  Done: False Info: False\n",
      "Step: 4  Action: 1  Reward: 1.0 NextState: [ 0.02249002  0.16802748 -0.00943934 -0.311451  ]  Done: False Info: False\n",
      "Step: 5  Action: 0  Reward: 1.0 NextState: [ 0.02585057 -0.02695873 -0.01566836 -0.02175983]  Done: False Info: False\n",
      "Step: 6  Action: 1  Reward: 1.0 NextState: [ 0.0253114   0.16838437 -0.01610356 -0.31934485]  Done: False Info: False\n",
      "Step: 7  Action: 0  Reward: 1.0 NextState: [ 0.02867909 -0.02650457 -0.02249045 -0.03178356]  Done: False Info: False\n",
      "Step: 8  Action: 1  Reward: 1.0 NextState: [ 0.028149    0.16893256 -0.02312613 -0.33147675]  Done: False Info: False\n",
      "Step: 9  Action: 0  Reward: 1.0 NextState: [ 0.03152765 -0.0258527  -0.02975566 -0.04617546]  Done: False Info: False\n",
      "Step: 10  Action: 1  Reward: 1.0 NextState: [ 0.03101059  0.16968301 -0.03067917 -0.34809607]  Done: False Info: False\n",
      "Step: 11  Action: 0  Reward: 1.0 NextState: [ 0.03440425 -0.02498945 -0.03764109 -0.06524304]  Done: False Info: False\n",
      "Step: 12  Action: 0  Reward: 1.0 NextState: [ 0.03390446 -0.21955208 -0.03894595  0.2153303 ]  Done: False Info: False\n",
      "Step: 13  Action: 0  Reward: 1.0 NextState: [ 0.02951342 -0.41409624 -0.03463935  0.49547803]  Done: False Info: False\n",
      "Step: 14  Action: 0  Reward: 1.0 NextState: [ 0.0212315  -0.60871303 -0.02472978  0.77704614]  Done: False Info: False\n",
      "Step: 15  Action: 0  Reward: 1.0 NextState: [ 0.00905724 -0.8034863  -0.00918886  1.061847  ]  Done: False Info: False\n",
      "Step: 16  Action: 1  Reward: 1.0 NextState: [-0.00701249 -0.6082439   0.01204808  0.7662942 ]  Done: False Info: False\n",
      "Step: 17  Action: 1  Reward: 1.0 NextState: [-0.01917737 -0.41328987  0.02737396  0.47742647]  Done: False Info: False\n",
      "Step: 18  Action: 0  Reward: 1.0 NextState: [-0.02744316 -0.6087874   0.03692249  0.7786098 ]  Done: False Info: False\n",
      "Step: 19  Action: 1  Reward: 1.0 NextState: [-0.03961891 -0.41419208  0.05249469  0.49776852]  Done: False Info: False\n",
      "Step: 20  Action: 0  Reward: 1.0 NextState: [-0.04790276 -0.61001337  0.06245006  0.8065222 ]  Done: False Info: False\n",
      "Step: 21  Action: 0  Reward: 1.0 NextState: [-0.06010302 -0.8059332   0.0785805   1.1181774 ]  Done: False Info: False\n",
      "Step: 22  Action: 1  Reward: 1.0 NextState: [-0.07622168 -0.61192524  0.10094405  0.85114247]  Done: False Info: False\n",
      "Step: 23  Action: 1  Reward: 1.0 NextState: [-0.08846019 -0.41831383  0.1179669   0.59183085]  Done: False Info: False\n",
      "Step: 24  Action: 1  Reward: 1.0 NextState: [-0.09682646 -0.22502373  0.12980351  0.33851263]  Done: False Info: False\n",
      "Step: 25  Action: 0  Reward: 1.0 NextState: [-0.10132694 -0.4217308   0.13657376  0.6691461 ]  Done: False Info: False\n",
      "Step: 26  Action: 1  Reward: 1.0 NextState: [-0.10976156 -0.22874552  0.14995669  0.42239177]  Done: False Info: False\n",
      "Step: 27  Action: 0  Reward: 1.0 NextState: [-0.11433647 -0.42563844  0.15840453  0.7583389 ]  Done: False Info: False\n",
      "Step: 28  Action: 1  Reward: 1.0 NextState: [-0.12284923 -0.23301263  0.1735713   0.5193941 ]  Done: False Info: False\n",
      "Step: 29  Action: 0  Reward: 1.0 NextState: [-0.12750949 -0.4300986   0.18395919  0.86135507]  Done: False Info: False\n",
      "Step: 30  Action: 0  Reward: 1.0 NextState: [-0.13611147 -0.6271847   0.20118628  1.2057756 ]  Done: False Info: False\n",
      "Step: 31  Action: 0  Reward: 1.0 NextState: [-0.14865516 -0.82425547  0.2253018   1.55417   ]  Done: True Info: False\n",
      "Episode 3 terminated after 32 timesteps with total reward 32.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 1  Reward: 1.0 NextState: [-0.013481    0.16495724  0.00451275 -0.33542117]  Done: False Info: False\n",
      "Step: 1  Action: 0  Reward: 1.0 NextState: [-0.01018185 -0.03022864 -0.00219567 -0.04131858]  Done: False Info: False\n",
      "Step: 2  Action: 0  Reward: 1.0 NextState: [-0.01078642 -0.22531904 -0.00302204  0.2506708 ]  Done: False Info: False\n",
      "Step: 3  Action: 0  Reward: 1.0 NextState: [-0.0152928  -0.4203977   0.00199137  0.542399  ]  Done: False Info: False\n",
      "Step: 4  Action: 0  Reward: 1.0 NextState: [-0.02370076 -0.6155476   0.01283935  0.8357087 ]  Done: False Info: False\n",
      "Step: 5  Action: 0  Reward: 1.0 NextState: [-0.03601171 -0.8108426   0.02955353  1.1324016 ]  Done: False Info: False\n",
      "Step: 6  Action: 1  Reward: 1.0 NextState: [-0.05222856 -0.6161197   0.05220156  0.8491323 ]  Done: False Info: False\n",
      "Step: 7  Action: 1  Reward: 1.0 NextState: [-0.06455095 -0.42174703  0.06918421  0.5733108 ]  Done: False Info: False\n",
      "Step: 8  Action: 0  Reward: 1.0 NextState: [-0.07298589 -0.6177673   0.08065043  0.88696176]  Done: False Info: False\n",
      "Step: 9  Action: 0  Reward: 1.0 NextState: [-0.08534124 -0.8138859   0.09838966  1.203869  ]  Done: False Info: False\n",
      "Step: 10  Action: 1  Reward: 1.0 NextState: [-0.10161896 -0.6201638   0.12246704  0.9435711 ]  Done: False Info: False\n",
      "Step: 11  Action: 0  Reward: 1.0 NextState: [-0.11402223 -0.81670386  0.14133847  1.2720885 ]  Done: False Info: False\n",
      "Step: 12  Action: 1  Reward: 1.0 NextState: [-0.13035631 -0.62364     0.16678023  1.0267957 ]  Done: False Info: False\n",
      "Step: 13  Action: 1  Reward: 1.0 NextState: [-0.1428291  -0.43108332  0.18731615  0.79077494]  Done: False Info: False\n",
      "Step: 14  Action: 1  Reward: 1.0 NextState: [-0.15145078 -0.23895927  0.20313165  0.56237936]  Done: False Info: False\n",
      "Step: 15  Action: 1  Reward: 1.0 NextState: [-0.15622996 -0.04717989  0.21437924  0.33993575]  Done: True Info: False\n",
      "Episode 4 terminated after 16 timesteps with total reward 16.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 1  Reward: 1.0 NextState: [ 0.01999199  0.22770335  0.0493958  -0.2978225 ]  Done: False Info: False\n",
      "Step: 1  Action: 0  Reward: 1.0 NextState: [0.02454606 0.03191334 0.04343935 0.01002075]  Done: False Info: False\n",
      "Step: 2  Action: 0  Reward: 1.0 NextState: [ 0.02518432 -0.1638038   0.04363976  0.31608668]  Done: False Info: False\n",
      "Step: 3  Action: 0  Reward: 1.0 NextState: [ 0.02190825 -0.3595193   0.0499615   0.62220645]  Done: False Info: False\n",
      "Step: 4  Action: 0  Reward: 1.0 NextState: [ 0.01471786 -0.555302    0.06240562  0.9301966 ]  Done: False Info: False\n",
      "Step: 5  Action: 1  Reward: 1.0 NextState: [ 0.00361182 -0.36107534  0.08100956  0.6577591 ]  Done: False Info: False\n",
      "Step: 6  Action: 1  Reward: 1.0 NextState: [-0.00360969 -0.16716887  0.09416474  0.391644  ]  Done: False Info: False\n",
      "Step: 7  Action: 0  Reward: 1.0 NextState: [-0.00695306 -0.36349225  0.10199762  0.71246797]  Done: False Info: False\n",
      "Step: 8  Action: 0  Reward: 1.0 NextState: [-0.01422291 -0.55986744  0.11624698  1.0354352 ]  Done: False Info: False\n",
      "Step: 9  Action: 1  Reward: 1.0 NextState: [-0.02542026 -0.36646673  0.13695568  0.78139174]  Done: False Info: False\n",
      "Step: 10  Action: 1  Reward: 1.0 NextState: [-0.03274959 -0.17346637  0.15258351  0.5347412 ]  Done: False Info: False\n",
      "Step: 11  Action: 0  Reward: 1.0 NextState: [-0.03621892 -0.37036756  0.16327834  0.8713472 ]  Done: False Info: False\n",
      "Step: 12  Action: 0  Reward: 1.0 NextState: [-0.04362627 -0.56728864  0.18070528  1.210591  ]  Done: False Info: False\n",
      "Step: 13  Action: 1  Reward: 1.0 NextState: [-0.05497205 -0.37489992  0.2049171   0.97954553]  Done: False Info: False\n",
      "Step: 14  Action: 0  Reward: 1.0 NextState: [-0.06247004 -0.57209116  0.22450802  1.3289688 ]  Done: True Info: False\n",
      "Episode 5 terminated after 15 timesteps with total reward 15.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 1  Reward: 1.0 NextState: [-0.04903921  0.23181133  0.04069721 -0.31002805]  Done: False Info: False\n",
      "Step: 1  Action: 0  Reward: 1.0 NextState: [-0.04440299  0.03613389  0.03449665 -0.00479324]  Done: False Info: False\n",
      "Step: 2  Action: 1  Reward: 1.0 NextState: [-0.04368031  0.23074456  0.03440078 -0.28639558]  Done: False Info: False\n",
      "Step: 3  Action: 1  Reward: 1.0 NextState: [-0.03906542  0.42535946  0.02867287 -0.56803334]  Done: False Info: False\n",
      "Step: 4  Action: 1  Reward: 1.0 NextState: [-0.03055823  0.6200677   0.01731221 -0.85154706]  Done: False Info: False\n",
      "Step: 5  Action: 0  Reward: 1.0 NextState: [-1.8156875e-02  4.2471409e-01  2.8126547e-04 -5.5347097e-01]  Done: False Info: False\n",
      "Step: 6  Action: 1  Reward: 1.0 NextState: [-0.00966259  0.6198321  -0.01078815 -0.8460652 ]  Done: False Info: False\n",
      "Step: 7  Action: 1  Reward: 1.0 NextState: [ 0.00273405  0.81509954 -0.02770946 -1.1421211 ]  Done: False Info: False\n",
      "Step: 8  Action: 0  Reward: 1.0 NextState: [ 0.01903604  0.6203505  -0.05055188 -0.8582551 ]  Done: False Info: False\n",
      "Step: 9  Action: 1  Reward: 1.0 NextState: [ 0.03144305  0.81612325 -0.06771698 -1.1663951 ]  Done: False Info: False\n",
      "Step: 10  Action: 1  Reward: 1.0 NextState: [ 0.04776551  1.012058   -0.09104489 -1.4795171 ]  Done: False Info: False\n",
      "Step: 11  Action: 1  Reward: 1.0 NextState: [ 0.06800667  1.2081656  -0.12063523 -1.7991905 ]  Done: False Info: False\n",
      "Step: 12  Action: 1  Reward: 1.0 NextState: [ 0.09216999  1.404413   -0.15661904 -2.126803  ]  Done: False Info: False\n",
      "Step: 13  Action: 1  Reward: 1.0 NextState: [ 0.12025824  1.6007067  -0.19915509 -2.4634976 ]  Done: False Info: False\n",
      "Step: 14  Action: 0  Reward: 1.0 NextState: [ 0.15227237  1.4077473  -0.24842505 -2.2379448 ]  Done: True Info: False\n",
      "Episode 6 terminated after 15 timesteps with total reward 15.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 1  Reward: 1.0 NextState: [ 0.03806487  0.21618167 -0.01927547 -0.30771726]  Done: False Info: False\n",
      "Step: 1  Action: 1  Reward: 1.0 NextState: [ 0.0423885   0.4115729  -0.02542982 -0.6064163 ]  Done: False Info: False\n",
      "Step: 2  Action: 0  Reward: 1.0 NextState: [ 0.05061996  0.21681558 -0.03755814 -0.32185033]  Done: False Info: False\n",
      "Step: 3  Action: 1  Reward: 1.0 NextState: [ 0.05495627  0.41245168 -0.04399515 -0.626137  ]  Done: False Info: False\n",
      "Step: 4  Action: 0  Reward: 1.0 NextState: [ 0.06320531  0.21797058 -0.05651789 -0.34762806]  Done: False Info: False\n",
      "Step: 5  Action: 1  Reward: 1.0 NextState: [ 0.06756472  0.41384897 -0.06347045 -0.6575839 ]  Done: False Info: False\n",
      "Step: 6  Action: 1  Reward: 1.0 NextState: [ 0.0758417   0.6097943  -0.07662213 -0.9695579 ]  Done: False Info: False\n",
      "Step: 7  Action: 1  Reward: 1.0 NextState: [ 0.08803758  0.8058565  -0.09601329 -1.2852932 ]  Done: False Info: False\n",
      "Step: 8  Action: 1  Reward: 1.0 NextState: [ 0.10415471  1.0020605  -0.12171915 -1.6064284 ]  Done: False Info: False\n",
      "Step: 9  Action: 0  Reward: 1.0 NextState: [ 0.12419593  0.8085701  -0.15384772 -1.3540372 ]  Done: False Info: False\n",
      "Step: 10  Action: 1  Reward: 1.0 NextState: [ 0.14036733  1.0052518  -0.18092845 -1.690628  ]  Done: False Info: False\n",
      "Step: 11  Action: 0  Reward: 1.0 NextState: [ 0.16047236  0.8126229  -0.21474102 -1.4593045 ]  Done: True Info: False\n",
      "Episode 7 terminated after 12 timesteps with total reward 12.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 1  Reward: 1.0 NextState: [-0.00439697  0.22287057  0.02004012 -0.24695013]  Done: False Info: False\n",
      "Step: 1  Action: 0  Reward: 1.0 NextState: [6.0436822e-05 2.7468221e-02 1.5101120e-02 5.1985923e-02]  Done: False Info: False\n",
      "Step: 2  Action: 0  Reward: 1.0 NextState: [ 0.0006098  -0.16786698  0.01614084  0.34939486]  Done: False Info: False\n",
      "Step: 3  Action: 0  Reward: 1.0 NextState: [-0.00274754 -0.36321473  0.02312873  0.6471235 ]  Done: False Info: False\n",
      "Step: 4  Action: 1  Reward: 1.0 NextState: [-0.01001183 -0.16842252  0.03607121  0.36181262]  Done: False Info: False\n",
      "Step: 5  Action: 1  Reward: 1.0 NextState: [-0.01338028  0.02616865  0.04330746  0.08071836]  Done: False Info: False\n",
      "Step: 6  Action: 0  Reward: 1.0 NextState: [-0.01285691 -0.1695465   0.04492183  0.38674423]  Done: False Info: False\n",
      "Step: 7  Action: 0  Reward: 1.0 NextState: [-0.01624784 -0.3652764   0.05265671  0.6932455 ]  Done: False Info: False\n",
      "Step: 8  Action: 1  Reward: 1.0 NextState: [-0.02355337 -0.17092294  0.06652162  0.41759327]  Done: False Info: False\n",
      "Step: 9  Action: 0  Reward: 1.0 NextState: [-0.02697183 -0.36692142  0.07487348  0.7304837 ]  Done: False Info: False\n",
      "Step: 10  Action: 0  Reward: 1.0 NextState: [-0.03431026 -0.56299394  0.08948316  1.0457608 ]  Done: False Info: False\n",
      "Step: 11  Action: 1  Reward: 1.0 NextState: [-0.04557014 -0.36916637  0.11039837  0.78245556]  Done: False Info: False\n",
      "Step: 12  Action: 0  Reward: 1.0 NextState: [-0.05295346 -0.56561846  0.12604748  1.107731  ]  Done: False Info: False\n",
      "Step: 13  Action: 0  Reward: 1.0 NextState: [-0.06426583 -0.7621513   0.1482021   1.4371513 ]  Done: False Info: False\n",
      "Step: 14  Action: 1  Reward: 1.0 NextState: [-0.07950886 -0.5691342   0.17694513  1.1942115 ]  Done: False Info: False\n",
      "Step: 15  Action: 1  Reward: 1.0 NextState: [-0.09089154 -0.37668863  0.20082936  0.9618013 ]  Done: False Info: False\n",
      "Step: 16  Action: 1  Reward: 1.0 NextState: [-0.09842531 -0.18474889  0.22006539  0.73832595]  Done: True Info: False\n",
      "Episode 8 terminated after 17 timesteps with total reward 17.0\n",
      "El episodio no se completó exitosamente.\n",
      "Step: 0  Action: 1  Reward: 1.0 NextState: [ 0.02449585  0.19330825  0.01308619 -0.3374252 ]  Done: False Info: False\n",
      "Step: 1  Action: 0  Reward: 1.0 NextState: [ 0.02836202 -0.00199746  0.00633769 -0.04064449]  Done: False Info: False\n",
      "Step: 2  Action: 1  Reward: 1.0 NextState: [ 0.02832207  0.19303304  0.0055248  -0.3313211 ]  Done: False Info: False\n",
      "Step: 3  Action: 1  Reward: 1.0 NextState: [ 0.03218273  0.38807592 -0.00110162 -0.62225664]  Done: False Info: False\n",
      "Step: 4  Action: 1  Reward: 1.0 NextState: [ 0.03994425  0.5832132  -0.01354676 -0.91528636]  Done: False Info: False\n",
      "Step: 5  Action: 0  Reward: 1.0 NextState: [ 0.05160851  0.38827708 -0.03185248 -0.62689155]  Done: False Info: False\n",
      "Step: 6  Action: 0  Reward: 1.0 NextState: [ 0.05937405  0.19361387 -0.04439032 -0.34440792]  Done: False Info: False\n",
      "Step: 7  Action: 0  Reward: 1.0 NextState: [ 0.06324633 -0.00084941 -0.05127848 -0.0660468 ]  Done: False Info: False\n",
      "Step: 8  Action: 0  Reward: 1.0 NextState: [ 0.06322934 -0.1952001  -0.05259941  0.21002677]  Done: False Info: False\n",
      "Step: 9  Action: 1  Reward: 1.0 NextState: [ 0.05932534  0.00063294 -0.04839887 -0.09877363]  Done: False Info: False\n",
      "Step: 10  Action: 0  Reward: 1.0 NextState: [ 0.059338   -0.19376315 -0.05037435  0.17825533]  Done: False Info: False\n",
      "Step: 11  Action: 0  Reward: 1.0 NextState: [ 0.05546274 -0.38812938 -0.04680924  0.45463106]  Done: False Info: False\n",
      "Step: 12  Action: 1  Reward: 1.0 NextState: [ 0.04770015 -0.1923779  -0.03771662  0.14756855]  Done: False Info: False\n",
      "Step: 13  Action: 1  Reward: 1.0 NextState: [ 0.04385259  0.0032633  -0.03476525 -0.15677059]  Done: False Info: False\n",
      "Step: 14  Action: 1  Reward: 1.0 NextState: [ 0.04391786  0.19886531 -0.03790066 -0.46021524]  Done: False Info: False\n",
      "Step: 15  Action: 0  Reward: 1.0 NextState: [ 0.04789516  0.004299   -0.04710497 -0.17971548]  Done: False Info: False\n",
      "Step: 16  Action: 0  Reward: 1.0 NextState: [ 0.04798114 -0.19011834 -0.05069927  0.0977433 ]  Done: False Info: False\n",
      "Step: 17  Action: 1  Reward: 1.0 NextState: [ 0.04417878  0.0056922  -0.04874441 -0.21049431]  Done: False Info: False\n",
      "Step: 18  Action: 1  Reward: 1.0 NextState: [ 0.04429262  0.201476   -0.0529543  -0.51814634]  Done: False Info: False\n",
      "Step: 19  Action: 0  Reward: 1.0 NextState: [ 0.04832214  0.00713804 -0.06331722 -0.24260934]  Done: False Info: False\n",
      "Step: 20  Action: 1  Reward: 1.0 NextState: [ 0.0484649   0.20310459 -0.06816941 -0.55457294]  Done: False Info: False\n",
      "Step: 21  Action: 0  Reward: 1.0 NextState: [ 0.05252699  0.00900272 -0.07926087 -0.28412268]  Done: False Info: False\n",
      "Step: 22  Action: 0  Reward: 1.0 NextState: [ 0.05270705 -0.18490456 -0.08494332 -0.01745322]  Done: False Info: False\n",
      "Step: 23  Action: 0  Reward: 1.0 NextState: [ 0.04900895 -0.3787121  -0.08529238  0.2472666 ]  Done: False Info: False\n",
      "Step: 24  Action: 1  Reward: 1.0 NextState: [ 0.04143471 -0.18248214 -0.08034705 -0.0710539 ]  Done: False Info: False\n",
      "Step: 25  Action: 0  Reward: 1.0 NextState: [ 0.03778507 -0.37636575 -0.08176813  0.19523667]  Done: False Info: False\n",
      "Step: 26  Action: 1  Reward: 1.0 NextState: [ 0.03025776 -0.18017516 -0.0778634  -0.12207903]  Done: False Info: False\n",
      "Step: 27  Action: 1  Reward: 1.0 NextState: [ 0.02665425  0.01597099 -0.08030498 -0.43827555]  Done: False Info: False\n",
      "Step: 28  Action: 0  Reward: 1.0 NextState: [ 0.02697367 -0.17792794 -0.08907049 -0.17194875]  Done: False Info: False\n",
      "Step: 29  Action: 0  Reward: 1.0 NextState: [ 0.02341511 -0.37166965 -0.09250946  0.09135965]  Done: False Info: False\n",
      "Step: 30  Action: 1  Reward: 1.0 NextState: [ 0.01598172 -0.17535186 -0.09068228 -0.22901687]  Done: False Info: False\n",
      "Step: 31  Action: 0  Reward: 1.0 NextState: [ 0.01247468 -0.3690688  -0.09526261  0.03374057]  Done: False Info: False\n",
      "Step: 32  Action: 0  Reward: 1.0 NextState: [ 0.00509331 -0.56270474 -0.0945878   0.29491264]  Done: False Info: False\n",
      "Step: 33  Action: 1  Reward: 1.0 NextState: [-0.00616079 -0.36637047 -0.08868954 -0.02603964]  Done: False Info: False\n",
      "Step: 34  Action: 0  Reward: 1.0 NextState: [-0.0134882  -0.560116   -0.08921034  0.23739585]  Done: False Info: False\n",
      "Step: 35  Action: 1  Reward: 1.0 NextState: [-0.02469052 -0.3638403  -0.08446242 -0.08203998]  Done: False Info: False\n",
      "Step: 36  Action: 1  Reward: 1.0 NextState: [-0.03196732 -0.16761552 -0.08610322 -0.40013033]  Done: False Info: False\n",
      "Step: 37  Action: 1  Reward: 1.0 NextState: [-0.03531963  0.02861556 -0.09410582 -0.7186696 ]  Done: False Info: False\n",
      "Step: 38  Action: 0  Reward: 1.0 NextState: [-0.03474732 -0.16508693 -0.10847922 -0.45702776]  Done: False Info: False\n",
      "Step: 39  Action: 1  Reward: 1.0 NextState: [-0.03804906  0.03138802 -0.11761978 -0.78183824]  Done: False Info: False\n",
      "Step: 40  Action: 1  Reward: 1.0 NextState: [-0.0374213   0.22791329 -0.13325654 -1.1090899 ]  Done: False Info: False\n",
      "Step: 41  Action: 0  Reward: 1.0 NextState: [-0.03286303  0.03476992 -0.15543833 -0.8610049 ]  Done: False Info: False\n",
      "Step: 42  Action: 1  Reward: 1.0 NextState: [-0.03216764  0.23162815 -0.17265843 -1.1982473 ]  Done: False Info: False\n",
      "Step: 43  Action: 0  Reward: 1.0 NextState: [-0.02753507  0.03910859 -0.19662338 -0.96427137]  Done: False Info: False\n",
      "Step: 44  Action: 1  Reward: 1.0 NextState: [-0.0267529   0.23625053 -0.21590881 -1.3117219 ]  Done: True Info: False\n",
      "Episode 9 terminated after 45 timesteps with total reward 45.0\n",
      "El episodio no se completó exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "def Random_games():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    # Each of this episode is its own game.\n",
    "    for episode in range(10):\n",
    "        env.reset()\n",
    "        total_reward = 0\n",
    "        # this is each frame, up to 500...but we wont make it that far with random.\n",
    "        for t in range(500):\n",
    "            # This will display the environment\n",
    "            # Only display if you really want to see it.\n",
    "            # Takes much longer to display it.\n",
    "            env.render()\n",
    "            \n",
    "            # This will just create a sample action in any environment.\n",
    "            # In this environment, the action can be 0 or 1, which is left or right\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # this executes the environment with an action, \n",
    "            # and returns the observation of the environment, \n",
    "            # the reward, if the env is over, and other info.\n",
    "            #print(env.step(action))\n",
    "            next_state, reward, done, info, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            # lets print everything in one line:\n",
    "             # Print information\n",
    "            print(\"Step:\", t, \" Action:\", action, \" Reward:\", reward, \"NextState:\", next_state, \" Done:\", done, \"Info:\", info)\n",
    "            \n",
    "            if done:\n",
    "                print(\"Episode\", episode, \"terminated after\", t+1, \"timesteps with total reward\", total_reward)\n",
    "                if total_reward>=500:\n",
    "                    print(\"El episodio se ha completado de manera exitosa.\")\n",
    "                else:\n",
    "                    print(\"El episodio no se completó exitosamente.\")\n",
    "                break  # Exit the loop if the episode is done\n",
    "                \n",
    "Random_games()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.utils.generic_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Activation\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DQNAgent, SARSAAgent\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\rl\\agents\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdqn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DQNAgent, NAFAgent, ContinuousDQNAgent\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mddpg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DDPGAgent\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CEMAgent\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\rl\\agents\\dqn.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lambda, Input, Layer, Dense\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EpsGreedyQPolicy, GreedyQPolicy\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\rl\\core.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m History\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     CallbackList,\n\u001b[0;32m     10\u001b[0m     TestLogger,\n\u001b[0;32m     11\u001b[0m     TrainEpisodeLogger,\n\u001b[0;32m     12\u001b[0m     TrainIntervalLogger,\n\u001b[0;32m     13\u001b[0m     Visualizer\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAgent\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124;03m\"\"\"Abstract base class for all implemented agents.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    Each agent interacts with the environment (as defined by the `Env` class) by first observing the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m        processor (`Processor` instance): See [Processor](#processor) for details.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\rl\\callbacks.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m KERAS_VERSION\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback \u001b[38;5;28;01mas\u001b[39;00m KerasCallback, CallbackList \u001b[38;5;28;01mas\u001b[39;00m KerasCallbackList\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Progbar\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCallback\u001b[39;00m(KerasCallback):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_env\u001b[39m(\u001b[38;5;28mself\u001b[39m, env):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.utils.generic_utils'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent, SARSAAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(state_size, action_size, hidden_units):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_sarsa(hidden_units):\n",
    "    model = build_model(env.observation_space.shape[0], env.action_space.n, hidden_units)\n",
    "    policy = EpsGreedyQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    sarsa = SARSAAgent(model=model, nb_actions=env.action_space.n, nb_steps_warmup=10, policy=policy, memory=memory)\n",
    "    sarsa.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "    return sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_network(hidden_units):\n",
    "    model = build_model(env.observation_space.shape[0], env.action_space.n, hidden_units)\n",
    "    policy = EpsGreedyQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, nb_actions=env.action_space.n, memory=memory, nb_steps_warmup=50, target_model_update=1e-2, policy=policy)\n",
    "    dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EpsGreedyQPolicy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m agents \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeepSarsa_4\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mdeep_sarsa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeepSarsa_16\u001b[39m\u001b[38;5;124m'\u001b[39m: deep_sarsa(\u001b[38;5;241m16\u001b[39m),\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDQN_4\u001b[39m\u001b[38;5;124m'\u001b[39m: deep_q_network(\u001b[38;5;241m4\u001b[39m),\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDQN_16\u001b[39m\u001b[38;5;124m'\u001b[39m: deep_q_network(\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m      6\u001b[0m }\n\u001b[0;32m      8\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, agent \u001b[38;5;129;01min\u001b[39;00m agents\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn [8], line 3\u001b[0m, in \u001b[0;36mdeep_sarsa\u001b[1;34m(hidden_units)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeep_sarsa\u001b[39m(hidden_units):\n\u001b[0;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m build_model(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn, hidden_units)\n\u001b[1;32m----> 3\u001b[0m     policy \u001b[38;5;241m=\u001b[39m \u001b[43mEpsGreedyQPolicy\u001b[49m()\n\u001b[0;32m      4\u001b[0m     memory \u001b[38;5;241m=\u001b[39m SequentialMemory(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m, window_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m     sarsa \u001b[38;5;241m=\u001b[39m SARSAAgent(model\u001b[38;5;241m=\u001b[39mmodel, nb_actions\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn, nb_steps_warmup\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, policy\u001b[38;5;241m=\u001b[39mpolicy, memory\u001b[38;5;241m=\u001b[39mmemory)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EpsGreedyQPolicy' is not defined"
     ]
    }
   ],
   "source": [
    "agents = {\n",
    "    'DeepSarsa_4': deep_sarsa(4),\n",
    "    'DeepSarsa_16': deep_sarsa(16),\n",
    "    'DQN_4': deep_q_network(4),\n",
    "    'DQN_16': deep_q_network(16)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, agent in agents.items():\n",
    "    agent.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "    results[name] = agent.test(env, nb_episodes=10, visualize=True)\n",
    "    # Guardar modelo y cargar para visualización si es necesario\n",
    "    agent.save_weights(f'{name}_weights.h5f', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'dict' and 'dict'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m sarsa_agent \u001b[38;5;241m=\u001b[39m SarsaAgent(num_states, num_actions)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Entrenar el agente SARSA\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m \u001b[43mtrain_sarsa\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msarsa_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Obtener la política del agente\u001b[39;00m\n\u001b[0;32m     68\u001b[0m policy \u001b[38;5;241m=\u001b[39m get_policy(sarsa_agent\u001b[38;5;241m.\u001b[39mq_table)\n",
      "Cell \u001b[1;32mIn[13], line 40\u001b[0m, in \u001b[0;36mtrain_sarsa\u001b[1;34m(env, agent, num_episodes)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m     39\u001b[0m     state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 40\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mdiscretize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[0;32m     42\u001b[0m     terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m, in \u001b[0;36mdiscretize_state\u001b[1;34m(state, num_states)\u001b[0m\n\u001b[0;32m     28\u001b[0m num_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(num_states \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m)))  \u001b[38;5;66;03m# Calcular el número de bins en base a la raíz cuarta del número de estados\u001b[39;00m\n\u001b[0;32m     29\u001b[0m bins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4.8\u001b[39m, \u001b[38;5;241m4.8\u001b[39m, num_bins),          \u001b[38;5;66;03m# Car position\u001b[39;00m\n\u001b[0;32m     30\u001b[0m                  np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, num_bins),              \u001b[38;5;66;03m# Car velocity\u001b[39;00m\n\u001b[0;32m     31\u001b[0m                  np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.418\u001b[39m, \u001b[38;5;241m0.418\u001b[39m, num_bins),     \u001b[38;5;66;03m# Pole angle\u001b[39;00m\n\u001b[0;32m     32\u001b[0m                  np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, num_bins)])            \u001b[38;5;66;03m# Pole velocity\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m discretized_state \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigitize\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(discretized_state)\n",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     28\u001b[0m num_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(num_states \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m)))  \u001b[38;5;66;03m# Calcular el número de bins en base a la raíz cuarta del número de estados\u001b[39;00m\n\u001b[0;32m     29\u001b[0m bins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4.8\u001b[39m, \u001b[38;5;241m4.8\u001b[39m, num_bins),          \u001b[38;5;66;03m# Car position\u001b[39;00m\n\u001b[0;32m     30\u001b[0m                  np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, num_bins),              \u001b[38;5;66;03m# Car velocity\u001b[39;00m\n\u001b[0;32m     31\u001b[0m                  np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.418\u001b[39m, \u001b[38;5;241m0.418\u001b[39m, num_bins),     \u001b[38;5;66;03m# Pole angle\u001b[39;00m\n\u001b[0;32m     32\u001b[0m                  np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, num_bins)])            \u001b[38;5;66;03m# Pole velocity\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m discretized_state \u001b[38;5;241m=\u001b[39m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigitize\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(state)]\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(discretized_state)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdigitize\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:5614\u001b[0m, in \u001b[0;36mdigitize\u001b[1;34m(x, bins, right)\u001b[0m\n\u001b[0;32m   5612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bins) \u001b[38;5;241m-\u001b[39m _nx\u001b[38;5;241m.\u001b[39msearchsorted(bins[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], x, side\u001b[38;5;241m=\u001b[39mside)\n\u001b[0;32m   5613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 5614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1413\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(a, v, side, sorter)\u001b[0m\n\u001b[0;32m   1345\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[0;32m   1349\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1411\u001b[0m \n\u001b[0;32m   1412\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearchsorted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:66\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class SarsaAgent:\n",
    "    def __init__(self, num_states, num_actions, discount_factor=0.99, alpha=0.1, epsilon=0.1):\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "        self.discount_factor = discount_factor\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_init = epsilon\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(len(self.q_table[state]))\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state, next_action):\n",
    "        predict = self.q_table[state, action]\n",
    "        target = reward + self.discount_factor * self.q_table[next_state, next_action]\n",
    "        self.q_table[state, action] += self.alpha * (target - predict)\n",
    "\n",
    "    def reset_epsilon(self):\n",
    "        self.epsilon = self.epsilon_init\n",
    "\n",
    "# Función para discretizar el espacio de estados\n",
    "def discretize_state(state, num_states):\n",
    "    num_bins = int(round(num_states ** (1/4)))  # Calcular el número de bins en base a la raíz cuarta del número de estados\n",
    "    bins = np.array([np.linspace(-4.8, 4.8, num_bins),          # Car position\n",
    "                     np.linspace(-5, 5, num_bins),              # Car velocity\n",
    "                     np.linspace(-0.418, 0.418, num_bins),     # Pole angle\n",
    "                     np.linspace(-5, 5, num_bins)])            # Pole velocity\n",
    "    discretized_state = [np.digitize(s, bins[i]) - 1 for i, s in enumerate(state)]\n",
    "    return tuple(discretized_state)\n",
    "\n",
    "# Función para entrenar el agente SARSA\n",
    "def train_sarsa(env, agent, num_episodes):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = discretize_state(state, num_states)\n",
    "        action = agent.choose_action(state)\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            next_state = discretize_state(next_state, num_states)\n",
    "            next_action = agent.choose_action(next_state)\n",
    "            agent.update_q_table(state, action, reward, next_state, next_action)\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        agent.reset_epsilon()\n",
    "\n",
    "# Función para obtener la política del agente\n",
    "def get_policy(q_table):\n",
    "    return np.argmax(q_table, axis=1)\n",
    "\n",
    "# Crear el entorno CartPole-v1\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Definir el agente SARSA\n",
    "num_states = 300\n",
    "num_actions = env.action_space.n\n",
    "sarsa_agent = SarsaAgent(num_states, num_actions)\n",
    "\n",
    "# Entrenar el agente SARSA\n",
    "train_sarsa(env, sarsa_agent, num_episodes=10000)\n",
    "\n",
    "# Obtener la política del agente\n",
    "policy = get_policy(sarsa_agent.q_table)\n",
    "print(\"Política aprendida:\", policy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
