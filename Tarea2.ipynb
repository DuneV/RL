{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning, 2024-01 \n",
    "# Tarea 2 - Programación Dinámica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Daniel Villar González, 201923374.  \n",
    "> Daniel Alvarez, 201911320."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considere la variación del juego escaleras y serpientes mostrada en la figura:**\n",
    "<center><img src=\"imagen-resource-h2.PNG\" alt=\"drawing\" width=\"400\"/>\n",
    "</center> \n",
    "\n",
    "- **La meta del jugador es ganar la partida llegando a una de las casillas marcadas en azul.**\n",
    "- **El jugador pierde la partida si cae en una de las casillas marcadas en rojo.**\n",
    "- **En cada jugada, antes de lanzar el dado, el jugador decide si quiere avanzar o retroceder el número de casillas indicadas por el dado.**\n",
    "- **El dado está cargado, con probabilidades p =[p1 p2 p3 p4 p5 p6].**\n",
    "- **En las casillas 1 y 100 la ficha rebota (si se supera el extremo, se avanza en la otra dirección la cantidad restante).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modelado de ambiente**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modele el esquema de juego tal que cumpla con las condiciones establecidas para el agente en el ambiente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snakeladders:\n",
    "    def __init__(self, p_dice, blue_coordinates, red_coordinates):\n",
    "        self.tablero = np.arange(1, 101)\n",
    "        self.blue_coordinates = blue_coordinates\n",
    "        self.red_coordinates = red_coordinates\n",
    "        self.p_dice= p_dice\n",
    "        self.actual_pos = 1\n",
    "    \n",
    "    def launch_dice(self):\n",
    "        return np.random.choice(np.arange(1, 7), p=self.p_dice)\n",
    "    \n",
    "    def bounce(self, new_position):\n",
    "        if new_position < 1:\n",
    "            return 1 - new_position\n",
    "        elif new_position > 100:\n",
    "            return 200 - new_position\n",
    "        else:\n",
    "            return new_position\n",
    "    \n",
    "    def move_token(self, dice):\n",
    "        new_position = self.actual_pos + dice\n",
    "        new_position = self.bounce(new_position)\n",
    "        if new_position in self.blue_coordinates:\n",
    "            print(\"BLUE BOX!!! WINNER\")\n",
    "            self.posicion_actual = new_position\n",
    "            return True\n",
    "        elif new_position in self.red_coordinates:\n",
    "            print(\"RED BOX!! LOSSER :(\")\n",
    "            self.posicion_actual = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.actual_pos = new_position\n",
    "            return True\n",
    "    \n",
    "    def play_game(self):\n",
    "        while self.actual_pos > 0 and self.actual_pos <= 100:\n",
    "            print(\"Your box is: \", self.actual_pos)\n",
    "            decision = input(\"Do you want to advance (A) or return (R)? \")\n",
    "            if decision.upper() == 'A':\n",
    "                dado = self.launch_dice()\n",
    "                print(\"Your dice is :\", dado)\n",
    "            elif decision.upper() == 'R':\n",
    "                dado = -self.launch_dice()\n",
    "                print(\"Your dice is:\", abs(dado))\n",
    "            elif decision.upper() == 'C':\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid decision!!\")\n",
    "                continue\n",
    "            if not self.move_token(dado):\n",
    "                print(\"End Game.\")\n",
    "                break\n",
    "\n",
    "\n",
    "p_dice = [0.1, 0.1, 0.2, 0.2, 0.2, 0.2]\n",
    "\n",
    "blue_coordinates = [100, 80]\n",
    "red_coordinates = [21, 37, 45, 65, 89]\n",
    "\n",
    "game = Snakeladders(p_dice, blue_coordinates, red_coordinates)\n",
    "game.play_game()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Punto 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modele este problema como un MDP. De detalladamente todos los elementos del MDP: estados, recompensas, acciones y p(s′, r | s, a) ∀s, s′, r, a, y factor de descuento γ.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Punto 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Escriba un módulo de Python para el MDP formulado. Su implementación debe tener métodos para:**\n",
    "\n",
    "- **Construir el MDP para una localización dada de las casillas azules y rojas, el vector p, y el factor de descuento γ.**\n",
    "- **Dada una política π (estocástica o determinística), calcular la función de valor de estado usando programación dinámica.**\n",
    "- **Dada una política π (estocástica o determinística), determinar si esta política es óptima chequeando las ecuaciones de optimalidad de Bellman.**\n",
    "- **Hallar una política óptima para una instancia del MDP, usando iteración de política.**\n",
    "- **Hallar una política óptima para una instancia del MDP, usando iteración de valor.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Punto 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilice su implementación de programación dinámica para hallar la función de valor de:**\n",
    "\n",
    "**a) La política que siempre avanza.**\n",
    "\n",
    "**b) La política que avanza con probabilidad 0,7.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Punto 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilice su implementación de iteración de política para encontrar una política óptima para la configuración mostrada en la figura para tres vectores p generados aleatoriamente. Chequee que la política encontrada en cada caso es efectivamente óptima. Muestre la política óptima en cada caso (acción en cada casilla). Repita el experimento pero ahora utilizando iteración de valor (usando los mismos p). Experimente en ambos casos con diferentes valores de γ.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusiones**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
