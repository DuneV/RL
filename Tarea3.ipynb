{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning, 2024-01 \n",
    "# Tarea 3 - Programación Dinámica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Daniel Villar González, 201923374.  \n",
    "> Daniel Alvarez, 201911320."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Descripción de tarea**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Punto 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considere el MDP que usted formuló para modelar este problema en el taller anterior.\n",
    "Escriba un módulo de Python que permita la interacción de un agente con el ambiente\n",
    "para el MDP formulado. Su implementación debe:\n",
    "- Dada una acción en un estado, retornar la recompensa y el estado resultante en\n",
    "esa acción. Considere el caso especial del estado terminal.\n",
    "- Ejecutar una política arbitraria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construirMDP(casillasAzules, casillasRojas, p, factorDescuento):\n",
    "    if factorDescuento > 1 or factorDescuento <= 0:\n",
    "        return \"El valor de descuento no está entre los límites permitidos\"\n",
    "    matrizAdelante = []\n",
    "    for i in range(100):\n",
    "        matrizAdelante.append([0] * 100)\n",
    "    matrizRecompensasAdelante = []\n",
    "    for i in range(100):\n",
    "        matrizRecompensasAdelante.append([0] * 100)\n",
    "    matrizAtras = []\n",
    "    for i in range(100):\n",
    "        matrizAtras.append([0] * 100)\n",
    "    matrizRecompensasAtras= []\n",
    "    for i in range(100):\n",
    "        matrizRecompensasAtras.append([0] * 100)\n",
    "    recompensas = {}\n",
    "    for i in range(len(casillasAzules)):\n",
    "        recompensas[casillasAzules[i]] = 1\n",
    "    for i in range(len(casillasRojas)):\n",
    "        recompensas[casillasRojas[i]] = -1\n",
    "    transiciones = {8:26, 21:82, 43:77, 50:91, 54:93, 62:96, 66:87, 80:100, 98:28, 95:24, 92:51, 83:19, 73:1, 69:33, 64:36, 59:17, 55:7, 52:11, 48:9, 46:5, 44:22}\n",
    "    llaves_transiciones = transiciones.keys()\n",
    "    llaves_recompensas = recompensas.keys()\n",
    "    for i in range(len(matrizAdelante)): \n",
    "        for j in range(0, 6):\n",
    "            destino = i + j + 1 \n",
    "            if destino >= 100:  \n",
    "                destino = 99 - (destino - 100)  \n",
    "            if destino+1 in llaves_transiciones:\n",
    "                matrizAdelante[i][transiciones[destino+1]-1] += p[j]  \n",
    "                if destino+1 in llaves_recompensas:\n",
    "                    matrizRecompensasAdelante[i][transiciones[destino+1]-1] += recompensas[destino+1]\n",
    "            else:\n",
    "                matrizAdelante[i][destino] += p[j] \n",
    "                if destino+1 in llaves_recompensas:\n",
    "                    matrizRecompensasAdelante[i][destino] += recompensas[destino+1]\n",
    "    for i in range(len(matrizAtras)): \n",
    "        for j in range(0, 6):\n",
    "            destino = i - j \n",
    "            if destino <= 0:  \n",
    "                destino = abs(destino) + 1\n",
    "            if destino in llaves_transiciones:\n",
    "                matrizAtras[i][transiciones[destino]-1] += p[j]  \n",
    "                if destino in llaves_recompensas:\n",
    "                    matrizRecompensasAtras[i][transiciones[destino]-1] += recompensas[destino]\n",
    "            else:\n",
    "                matrizAtras[i][destino-1] += p[j] \n",
    "                if destino in llaves_recompensas:\n",
    "                    matrizRecompensasAtras[i][destino-1] += recompensas[destino]\n",
    "    return matrizAdelante, matrizAtras, matrizRecompensasAdelante, matrizRecompensasAtras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'adelante', 0, 1), (5, 'adelante', 0, 5), (11, 'adelante', 0, 11), (14, 'adelante', 0, 14), (81, 'adelante', 0, 81), (78, 'atras', 1, 78), (74, 'atras', 0, 74), (69, 'atras', 0, 69), (66, 'atras', 0, 66), (60, 'atras', 0, 60)]\n"
     ]
    }
   ],
   "source": [
    "class MDPenv:\n",
    "    def __init__(self, casillasAzules, casillasRojas, p, factorDescuento):\n",
    "        self.matrizAdelante, self.matrizAtras, self.matrizRecompensasAdelante, self.matrizRecompensasAtras = construirMDP(casillasAzules, casillasRojas, p, factorDescuento)\n",
    "        self.estado_actual = 0\n",
    "        self.factorDescuento = factorDescuento\n",
    "        self.terminal_state = casillasAzules\n",
    "\n",
    "    def accion(self, movimiento):\n",
    "        \"\"\"\n",
    "        Toma la acción dependiendo de la decisión del agente\n",
    "        sabemos que si se encuentra en estado terminal la recompensa es 0, y el\n",
    "        nuevo estado es el mismo.\n",
    "        \"\"\"\n",
    "        for state in self.terminal_state:\n",
    "            if self.estado_actual == state:\n",
    "                return 0, self.estado_actual\n",
    "\n",
    "        # Decidir la matriz de transición y de recompensas a usar basado en la acción\n",
    "        if movimiento == 'adelante':\n",
    "            transiciones = self.matrizAdelante[self.estado_actual]\n",
    "            recompensas = self.matrizRecompensasAdelante[self.estado_actual]\n",
    "        elif movimiento == 'atras':\n",
    "            transiciones = self.matrizAtras[self.estado_actual]\n",
    "            recompensas = self.matrizRecompensasAtras[self.estado_actual]\n",
    "        # else:\n",
    "        #     raise ValueError(\"Movimiento no válido. Use 'adelante' o 'atras'.\")\n",
    "        \n",
    "        # depende de los valores de la matriz de transicion que se obtiene con p \n",
    "        nuevo_estado = self._determinar_nuevo_estado(transiciones)\n",
    "        # recibe recompensa\n",
    "        recompensa = recompensas[nuevo_estado]\n",
    "        # aquí actualizamos el estado :)\n",
    "        self.estado_actual = nuevo_estado\n",
    "        # devolvemos la dos cosas que ve tomada la acción recompensa y nuevo estado\n",
    "        return recompensa, nuevo_estado\n",
    "    \n",
    "    # aqui se toma una decisión aleatoria de los siguientes estados en función de la matriz de transición \n",
    "    # esto tiene sentido para el agente pero se debe cambiar para SARSA porque parece que no tiene aprendizaje\n",
    "    def _determinar_nuevo_estado(self, transiciones):\n",
    "        return random.choices(range(100), weights=transiciones)[0]\n",
    "\n",
    "    def ejecutar_politica(self, politica, pasos):\n",
    "        \"\"\"\n",
    "        Ejecuta una política dada durante un número especificado de pasos.\n",
    "        política es una función que toma un estado y devuelve una acción.\n",
    "        \"\"\"\n",
    "        historia = []\n",
    "        for _ in range(pasos):\n",
    "            for state in self.terminal_state:\n",
    "                if self.estado_actual == state:\n",
    "                    break\n",
    "            accion = politica(self.estado_actual)\n",
    "            recompensa, estado = self.accion(accion)\n",
    "            historia.append((self.estado_actual, accion, recompensa, estado))\n",
    "        return historia\n",
    "    \n",
    "# TEST\n",
    "\n",
    "def politica_simple(estado):\n",
    "    return 'adelante' if estado < 50 else 'atras'\n",
    "\n",
    "mdp = MDPenv([99, 79], [22, 38, 44, 66, 88], [0.1, 0.1, 0.2, 0.3, 0.2, 0.1], 0.99)\n",
    "historia = mdp.ejecutar_politica(politica_simple, 10)\n",
    "print(historia)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Punto 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Punto 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    def __init__(self, num_states, discount_factor=0.99, alpha=0.5, epsilon=0.1):\n",
    "        self.q_table = np.zeros((num_states, 2))  # acciones\n",
    "        self.discount_factor = discount_factor\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return 'adelante' if np.random.rand() < 0.5 else 'atras'  # Acción aleatoria\n",
    "        else:\n",
    "            action_index = np.argmax(self.q_table[state])\n",
    "            return 'adelante' if action_index == 0 else 'atras'  # Mejor acción conocida\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state, next_action):\n",
    "        action_index = 0 if action == 'adelante' else 1\n",
    "        next_action_index = 0 if next_action == 'adelante' else 1\n",
    "        predict = self.q_table[state, action_index]\n",
    "        target = reward + self.discount_factor * self.q_table[next_state, next_action_index]\n",
    "        self.q_table[state, action_index] += self.alpha * (target - predict)\n",
    "\n",
    "def train_sarsa(mdp, agent, num_episodes):\n",
    "    for episode in range(num_episodes):\n",
    "        state = mdp.estado_actual\n",
    "        action = agent.choose_action(state)\n",
    "\n",
    "        while state != mdp.terminal_state:\n",
    "            reward, next_state = mdp.accion(action)\n",
    "            next_action = agent.choose_action(next_state)\n",
    "\n",
    "            agent.update_q_table(state, action, reward, next_state, next_action)\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "            if state == mdp.terminal_state:\n",
    "                break\n",
    "\n",
    "# Instanciación y entrenamiento\n",
    "mdp = MDPenv([2, 5, 10], [15, 25, 35], [0.1, 0.1, 0.2, 0.3, 0.2, 0.1], 0.99)\n",
    "num_states = 100\n",
    "sarsa_agent = SarsaAgent(num_states)\n",
    "train_sarsa(mdp, sarsa_agent, num_episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Punto 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Punto 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ambos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
